---
title: "MovieLens Project"
author: "Dr Daniel Mayenberger"
date: "May 2020"
header-includes:
   - \usepackage{hyperref}
   - \usepackage[usenames,dvipsnames]{xcolor}
   - \usepackage{amsmath}
   - \usepackage{tabularx}
output: 
    pdf_document:
        number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries and load data, include = FALSE}
# libraries
library(tidyverse)
library(lubridate)
library(caret)
library(gam)
library(ggplot2)
library(gridExtra)
# Set to TRUE for final version, is kept at FALSE for quicker compiling
# of drafts
FINAL_VERSION = FALSE

# load data from local repository
if(FINAL_VERSION) {
    if(!exists("edx") | !exists("validation")) {
        print("Loading data sets 'edx' and 'validation'...")
        load(file = "data//dslabs_movielens.Rdata")
        print("Done!")
    } else {
        print("Data 'edx' and 'validation' already loaded")
        }
} else {
    if(!exists("edx_1000") | !exists("validation_1000")) {
        print("Loading data sets 'edx_1000' and 'validation_1000'...")
        load(file = "data//dslabs_movielens_extract.Rdata")
        print("Done!")
    } else {
        print("Data 'edx_1000' and 'validation_1000' already loaded")
        }
    # assign to "full data" FOR TESTING
    edx <- edx_1000
    validation <- validation_1000
    }
```

<!--\newcommand{\blueref}[1]{\textcolor{blue}{\autoref{#1}}}-->
\newcommand{\blueref}[2]{\href{#1}{\textcolor{blue}{#2}}}
\newcommand{\greenref}[1]
{\color{green}\underline{{\color{black}\autoref{#1}}}\color{black}{}}
\newcommand{\greenuline}[1]
{\color{green}\underline{{\color{black}#1}}\color{black}{}}

# Executive Summary
Given over 10,000 movies and about 70,000 users that have rated some of these
movies on a rating scale from half a star to five stars, the goal is to predict
future ratings based on past ratings. The challenge is to do this for the over
700 millions different movie/user combinations  based on only 1.2% (9 millions)
of such combinations used for training the algorithm.

The available data of 10 million ratings are split into 9 million ratings to
train different algorithms and 1 million ratings to evaluate the performance of
these methods. Performance is measured by the root-mean squared error (RMSE) of
ratings.

Based on the RMSE, the model that performs best with an RMSE of 0.8648 is
\[
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K \delta_{i,k} \beta_k + 
\varepsilon_{u,i},
\]
where $Y_{u,i}$ is the rating by user $u$ of movie $i$, $\delta_{i,k}$ is
an indicator which is $1$ if the movie $i$ is of genre $k$ and zero otherwise,
$b_i$ is the movie effect, $b_u$ the user effect and $\beta_k$ the genre
effect.

# Introduction
The movielens project makes available millions of a movie ratings provided by
anonymised users.

From the movielens website information about the data on
\blueref{http://files.grouplens.org/datasets/movielens/ml-10m-README.html}
{this website} the following further information is provided: 

* The data set contains just over 10 million ratings by users of the online
movie recommender service MovieLens.
* Users were selected at random for inclusion. All users selected had rated at
least 20 movies. Each user is represented by an id, and no other information is
provided about users.

Denoting the rating by any of these users $u$ of a certain movie $i$, the task
is to predict ratings $Y_{u,i}$ of any such user/movie combination $(u,i)$. The
challenge is that there are overall 71,567 users and 10,681 movies (figures from
the website), yielding $71,567 \times 10,681 = 764,407,127$ while only 10
million data points, so only about 1.3% $(=10/764.4)$ such data, are available
in totality and 10% (1 million) of the data must be held out as test set,
leaving 1.2% $(=9/764.4)$ of data to train a rating prediction algorithm.

To do so, the data are examined and modelled in 
\greenref{methods} which is further broken down into:

* \greenref{methods_techniques} to elaborate on the techniques used, in 
particular those code later in the modelling \greenref{methods_modelling}.
* \greenref{methods_data_structure} to provide an overview of the basic
structure of the rating data.
* \greenref{methods_data_cleaning} to perform data cleaning.
* \greenref{methods_data_exp_vis} to visualise the most important 
properties of the ratings with respect to individual movies, their genres
or individual users. These properties then inspire the modelling methods
in the subseqeuent section.
* \greenref{methods_modelling} presents the models based on the most
salient data properties and calibrates any free modelling parameters.

The results of all models are summarised in \greenref{results} and the
conclusions are drawn in \greenref{conclusion}.

# Methods and Analysis {#methods}

## Process and Techniques Used {#methods_techniques}

Two modelling techniques will be described, first the regularisation in 
\greenref{method_regularisation} and then the local estimated scatterplot 
smoothing (LOESS) method in \greenref{method_loess}.

### Regularisation {#method_regularisation}
The modelling techniques employed are regularised least squared estimates.
As an example, a model of the form
\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]
for user $u$ and movie $i$ assumes that there is an overall average rating of
$\mu$ across all movies. The $b_i$ is based on the observation that some movies
are better rated than others, as will be shown in [Section
3.4.2](#data_movie_effect). 
However, as there are some movies that are rated by hundreds or more users, the
ratings of these movies are more reliable than the ratings of movies that are
only rated by a few users. To calculate such an effect, denote the number of
ratings awarded to movie $i$ by $n_i$, the overall number of ratings as $N$ and
the overall number of movies by $J$. Further, denote the overall average rating
with $\mu$.

Without any weighing of ratings for a movie by the number of ratings awarded to
it, the goal is to minimise the MSE given by
\[
MSE = \frac{1}{N} \sum_{i=1}^J \sum_{u=1}^{n_i} (y_{u,i} - \hat{y}_{u,i})^2.
\]
As for any user $u$ and movie $i$ the modelled rating is given by 
$\hat{y}_{u,i} = \mu + \hat{b}_i$, the MSE to minimise becomes
\[
MSE = \frac{1}{N} \sum_{i=1}^J \sum_{u=1}^{n_i} 
\left( y_{u,i} - (\mu + \hat{b}_i) \right)^2.
\]
The necessary condition for a minimum is $\frac{\partial MSE}{\partial b_j}=0$
for all $j=1,2, \ldots, J$ which leads to
\[
0 = -\frac{2}{N} \sum_{u=1}^{n_j} \left( y_{u,j} - (\mu + \hat{b}_j) \right)
\quad (j=1,\ldots,J).
\]
This is equivalent to
\[
\hat{b}_j = \frac{1}{n_j} \sum_{u=1}^{n_j}(y_{u,j} - \mu) \quad (j=1,\ldots,J).
\]
To penalise estimates for movies that are only rated by a few users, the
MSE to minimise then becomes
\[
MSE(\lambda) = \frac{1}{N} \left[
\sum_{i=1}^J \sum_{u=1}^{n_i} (y_{u,i} - \hat{y}_{u,i})^2 +
\lambda \sum_{i=1}^J b_i^2
\right]
\]
with a regularisation parameter $\lambda \geq 0$. Again, the necessary condition
for a minimum of MSE is $\frac{\partial MSE(\lambda)}{\partial b_j}=0$
for all $j=1,2, \ldots, J$ which implies
\[
0 = -\sum_{u=1}^{n_j} \left( y_{u,j} - (\mu + \hat{b}_j) \right) +
\lambda b_j
\quad (j=1,\ldots,J).
\]
Solving for $\hat{b}_j$ yields
\[
\hat{b}_j = \frac{1}{n_j+\lambda} 
\sum_{u=1}^{n_j}(y_{u,j} - \mu) \quad (j=1,\ldots,J).
\]
This solution is a minimum, since the second derivative (Hessian matrix) of the
MSE function is a diagonal matrix, as 
$\frac{\partial^2MSE}{\partial b_i \partial b_j} = 0$ for $i \neq j$ and
$\frac{\partial^2MSE}{\partial b_i^2} = 2\frac{n_i+\lambda}{N} > 0$. There are
only positive entries on the diagonal of this matrix, and with that it is
positively definitive. So the solution for $\hat{b}_j$ provided above is
a local minimum and as there are no other local optima (there is only one
point at which the first partial derivative vanishes), it is also a global
minimum.

### Local Estimated Scatterplot Smoothing (LOESS) {#method_loess}
Unlike standard local regression that fits a line to the whole data set,
the locally estimated scatterplot smoothing (LOESS) estimates a regression
line in a local window that is progressively moved through the data. The
parameter of LOESS is the width of this window.

Further details of the LOESS can be found on 
\blueref{https://en.wikipedia.org/wiki/Local_regression}
{this Wikipedia website}.

## Data Structure and Loading {#methods_data_structure}
The raw data is provided in two sets, called `edx` and `validation`.

### Sample Data {#data_sample_data}
To facilitate code testing, 1,000 samples of data have been extracted from
both datasets `edx` and `validation`, called `edx_1000` and `validation_1000`
respectively. This is achieved with the following code:

```{r data sample data, warning = FALSE}
set.seed(123, sample.kind = "Rounding")
edx_1000 <- sample_n(edx, size = 1000)
validation_1000 <- sample_n(validation, size = 1000)
```

### Basic Data Structure
Both `edx` and `validation` have the same structure that can be displayed with 
the `str` function and is listed below for `edx`:

|Column name   |Type      |First values                                                                                     |
|:---------|:---------|:-----------------------------------------------------------------------------------------------|
|userId    |integer   |1 1 1                                                                                           |
|movieId   |numeric   |122 185 292                                                                                     |
|rating    |numeric   |5 5 5                                                                                           |
|timestamp |integer   |838985046 838983525 838983421                                                                   |
|title     |character |Boomerang (1992) Net, The (1995) Outbreak (1995)                                                |
|genres    |character |Comedy&#124;Romance Action&#124;Crime&#124;Thriller Action&#124;Drama&#124;Sci-Fi&#124;Thriller |


The data sets are both tidy - the count of `NA` values is zero for both 
sets:
```{r data tidy}
edx %>% summarise_all(~sum(is.na(.)))
validation %>% summarise_all(~sum(is.na(.)))
```

The `userId`, `movieId` , `rating`, `title` and `genres` data can be used
directly in the format provided.

The `timestamp` column is in raw data format of seconds since 1 January 1970 
and will be converted into a date & time format.

## Data Cleaning {#methods_data_cleaning}
The `timestamp` column is converted to a date and time using the `as_datetime`
function for both the `edx` and the `validation` dataset and stored in 
a new column `ratingdate`. After the conversion the `timestamp` columns
are no longer required and are discarded.

```{r data conversion of date}
edx <- edx %>%
    mutate(ratingdate = as_datetime(timestamp)) %>%
    select(-timestamp)

validation <- validation %>%
    mutate(ratingdate = as_datetime(timestamp)) %>%
    select(-timestamp)
```

## Data Exploration and Visualisation {#methods_data_exp_vis}

### General Data Distribution Properties
For better readability, the code pieces for the subsequent graphs in this Section
are displayed in \hyperlink{appendix_figures}{\greenuline{Appendix A}}.

The `edx` data set contains 9,000,055 data points, consisting of 10,677
distinct movies and 69,878 different users:
```{r data count}
edx %>%
    summarise(n_movies = n_distinct(movieId),
              n_users = n_distinct(userId))
```
With that, the total number of diferent movie/user combinations is
$10,677 \times 69,878 = 746,087,406$, much more than the overall 10 million
data points from the combined `edx` and `validation` sets. While it is not
feasible with the memory capacity of a personal computer to visualise all of
the over 700 million combinations, it is possible to illustrate the sparsity
of the data set on a representative sample.

```{r, include = FALSE}
n_movies_sub <- length(unique(edx_1000$movieId))
n_users_sub <- length(unique(edx_1000$userId))
```


#### Data Sparsity {#data_sparsity_figure}
Within the sample of 1,000 ratings drawn as described in 
\greenref{data_sample_data} there are already `r n_movies_sub`
different movies and `r n_users_sub` different users. Plotting the
data points of this matrix shows that the data given are very sparse 
but at least evenly distributed among movies and users.

```{r data matrix of subset, echo = FALSE}
edx_1000 %>%
    mutate(rating = 1) %>%
    select(movieId, userId, rating) %>%
    spread(movieId, rating) %>% 
    column_to_rownames(var = "userId") %>% 
    as.matrix() %>% t() %>%
    image(x = 1:n_movies_sub, y = 1:n_users_sub, z = ., 
          xlab = "Movies", ylab = "Users",
          col = grey.colors(n = 1, start = 0, end = 1))
```

The code is shown \hyperlink{code_sparsity_figure}{\greenuline{here}}.

#### Ratings by Movie {#data_ratings_by_movie_figure}

There is a wide variety of the number of ratings by movie. Some movies are
rated only up to 10 times, while others are rated thousands of times:

```{r rating by movie, echo = FALSE}
edx %>%
    group_by(movieId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black") +
    labs(title = "Rating counts by movie", 
         x = "Number of Ratings (log scale)")
```

The code is shown \hyperlink{code_ratings_by_movie_figure}{\greenuline{here}}.

#### Ratings by User {#data_ratings_by_user_figure}

Similarly, there are users that only rate a few movies while other rate
hundreds or even thousands of movies:

```{r rating by user, echo = FALSE}
edx %>%
    group_by(userId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 40, col = "black") +
    labs(title = "Rating counts by user", 
         x = "Number of Ratings (log scale)")
```

The code is shown \hyperlink{code_ratings_by_user_figure}{\greenuline{here}}.

#### Distribution of Ratings {#data_ratings_distribution_figure}

The ratings most awared are three and four stars. In general, whole-star
ratings are more frequently given than half-star ratings:

```{r rating histogram, echo = FALSE}
edx %>%
    count(rating) %>%
    ggplot(aes(x = factor(rating), y = n)) +
    geom_bar(stat = "identity", width = 1, col = "black")+
    labs(title = "Distribution of ratings - total",
         x = "Rating", y = "Count") 
```

The code is displayed \hyperlink{code_ratings_distribution_figure}{\greenuline{here}}.

### Movie Effect {#data_movie_effect}
From public movie ratings such as [Rotten
Tomatoes](https://www.rottentomatoes.com/) it is known that some movies are in
general better rated than others. We group the ratings by movie ID to 
evaluate the same effect. In addition to the average rating for each movie,
the standard deviation of ratings for the same movie is visualised in parallel.

```{r dist rating by movie, echo = FALSE}
# distribution of ratings by movie
movie_avgs <- edx %>%
    group_by(movieId) %>%
    summarise(movie_avg = mean(rating),
              movie_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_bymovie = n()) %>%
    arrange(movie_avg) %>%
    mutate(row = row_number(movie_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_avg)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by movie")
p2 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_sd)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is \hyperlink{code_dist_ratings_by_movie}{\greenuline{here}}.

So there are indeed movies that, on average:

* are rated worse than other movies, on the left side of the chart.
* are rated better than other movies, towards the right of the chart.

At the same time, there is a high variability of this *movie effect*, 
illustrated by the broad range of the standard deviation in the 
lower half of the figure. Standard deviation of ratings by movie hovers around
the value of one (star).

### User Effect {#data_user_effect}
Similar to the movie effect, different users may have the tendency to award
higher or lower ratings, compared to other users.

```{r dist rating by user, echo = FALSE}
# distribution of ratings by user
user_avgs <- edx %>%
    group_by(userId) %>%
    summarise(user_avg = mean(rating),
              user_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_byuser = n()) %>%
    arrange(user_avg) %>%
    mutate(row = row_number(user_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- user_avgs %>%
    ggplot(aes(x = row, y = user_avg)) +
    geom_point() +
    labs(x = "User (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by user")
p2 <- user_avgs %>%
    ggplot(aes(x = row, y = user_sd)) +
    geom_point() +
    labs(x = "User (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is \hyperlink{code_dist_ratings_by_user}{\greenuline{here}}.

So similarly to the movie effect, there is a *user effect* with users that 
tend to assign movies lower rating on the left hand side of the chart and
users that rate movies more highly towards the right. In constrast to the
*movie effect* there is much higher variability of rating, consistent with
users differentiating between good and bad movies.


### Time of Rating {#data_time_effect}
Another feature that may influence the rating is the date and time at which
the rating was awarded. When averaging all ratings within a week and following
this average with a LOESS function, the following pattern emerges:

#### Overall Time Effect {#data_time_effect_total}
```{r rating by time, echo = FALSE}
edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    group_by(ratingdate_wk) %>%
    summarise(rating_wk = mean(rating)) %>%
    ggplot(aes(x = ratingdate_wk, y = rating_wk)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Rating by date",
         x = "Rating date (granularity of weeks)",
         y = "Rating")
```
  
The code for this chart is \hyperlink{code_ratings_by_time}{\greenuline{here}}.

So there is a small effect that is worth capturing, as a smooth function
of time. 

#### Residual Time Effect {#data_time_effect_res}

Since the effect will be modelled after movie effect and user
effect are accounted for, it is also informative to plot the residual
effect.

```{r residual time effect, echo = FALSE}
min_date = min(edx$ratingdate)
lambda_mur <- 5.0
# Calculate regularised movie user and time effect, see 
mu <- mean(edx$rating)
# Movie effect
b_i_tbl <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
# User effect
b_u_tbl <- edx %>% 
    left_join(b_i_tbl, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
# Residual time effect
d_ui_tbl <- edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
    left_join(b_i_tbl, by = "movieId") %>%
    left_join(b_u_tbl, by = "userId") %>%
    group_by(rating_wk) %>%
    summarise(d_ui = mean(rating - mu - b_i - b_u))

# plot the overall time effect once movie and user effect are accounted for
d_ui_tbl %>%
    ggplot(aes(x = as.numeric(rating_wk), y = d_ui)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Residual time effect of ratings",
         x = paste0("Weeks since ",format(min_date, "%d %b %Y")),
         y = "Residual rating effect (stars)")

```

The code for this figure can be found 
\hyperlink{code_ratings_by_time_res}{\greenuline{here}}.

In comparison to the total effect, the residual time effect is:

* much stronger in the first 2 years, after `r format(min_date, "%d %b %Y")`.
* considerable flatter after the the first 2 years.

### Genre Effect {#data_genre_effect}

As final feature, we investigate whether certain genres (or their combinations)
are rated differently from others. First, we determine whether genres have
sufficiently many ratings by examining the distribution of the number of
ratings by genre:

```{r genre number of ratings, echo = FALSE}
edx %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              nratings = n()) %>%
    arrange(nratings) %>%
    ggplot(aes(x = nratings)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black")
```

The code for this figure is shown \hyperlink{code_nratings_by_genre}{\greenuline{here}}.

So there are definitely sufficiently many genres with 1,000 or more ratings.
In the same way as for movies and users, we group the rating by genres for
all genres with over 1,000 rating and chart their average rating and standard 
deviation.

```{r ratings by genre, echo = FALSE}
p1 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_avg)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "Ratings by genres (with at least 1000 ratings)",
         x = "Genre (sorted by average rating)",
         y = "Average Rating")

p2 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_sd)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "",
         x = "Genre (sorted by average rating)",
         y = "Rating Standard Deviation")

grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is shown \hyperlink{code_ratings_by_genre}{\greenuline{here}}.

So the genre combination does have an effect on the rating, in a similar way
as movies and users do.

### Extrapolation Requirements Check
To predict movie ratings and calculate the final RMSE, it must be considered 
how the algorithm handles data in the `validation` data set to which it was
not trained in the `edx` set. It will be shown that there is no need for 
such extrapolation for the provided data sets.

By construction, the `validation` set only contains movies and users that 
are also in the `edx` set, with the `semi_join` statements provided in
the instructions. For verification, this code confirms that indeed no movies
or users from the `validation` set are missing from the `edx` set:

```{r}
validation %>%
    anti_join(edx, by = "movieId") %>%
    group_by(movieId, title) %>%
    summarise(n = n())

validation %>%
    anti_join(edx, by = "userId") %>%
    group_by(userId) %>%
    summarise(n = n())
```

In addition, all genres, and even more specifically, all their combinations
that are present in the `validation` set, are also found in the `edx` set, 
as this code shows:
```{r}
validation %>%
    anti_join(edx, by = "genres") %>%
    group_by(genres) %>%
    summarise(n = n())
```
So with the given data set, no extrapolation to movies, users or genres needs
to be made in predicting ratings and determining model performance.

## Modelling Approach {#methods_modelling}
The training of model parameters is generally done using k-fold 
cross-validation. To do this, the `edx` data is split into a training and
a validation set $k$ times. 

Graphically, k-fold validation can be illustrated by $k$ subsequent splits of
the overall `edx` set into training sets (shown in blue) and test sets (shown in
purple).

```{r kfold crossvalidation, echo = FALSE}
n_data <- nrow(edx)
validation_split <- data.frame(
    k = seq(1, 25, 1),
    C = seq(0, 24, 1) / 25 * n_data,
    B = rep(1/25 * n_data, time = 25))
validation_split <- validation_split %>%
    mutate(A = n_data - B - C)
validation_split <- validation_split %>%
    gather(set, value, -k) 

validation_split %>%
    ggplot(aes(x = k, y = value, fill = set)) +
    scale_fill_manual(values = c("#ACE5EE", "#FAF0BE", "#ACE5EE")) +
    theme(plot.margin = margin(0,0,0,0, "cm")) +
    geom_bar(stat = "identity", position = "stack", show.legend = FALSE) +
    labs(title = "Illustration of k-fold cross validation (k = 25)",
         y = "Values")
```
\definecolor{blizzardblue}{rgb}{0.67, 0.9, 0.93}
\definecolor{blond}{rgb}{0.98, 0.94, 0.75}

For example, the parameter $\lambda$ is applied to the
\colorbox{blizzardblue}{training data} and the RMSE that results from that
particular values of $\lambda$ is evaluated in the 
\colorbox{blond}{test data}.

The **performance** of each algorithm is evaluated using the RMSE which is
implemented in the function `RMSE_rating`:

```{r RMSE function}
RMSE_rating <- function(pred_ratings, true_ratings) {
    ifelse(length(pred_ratings) > 0,
           sqrt(mean((pred_ratings - true_ratings)^2)),
           NA)
    }
```

### Constant Value

This is the simplest baseline model that assumes that the rating is the
same every time. The model is
\[
Y_{u,i} = \mu + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
The parameter $\mu$ is estimated as average across all ratings:
\[
\hat{\mu} = \frac{1}{N} \sum_{u,i} y_{u,i}.
\]

The R implementation of this model is given by a function that returns the
same value for each record of the requested data set:
```{r estimator const}
predict_const <- function(newdata) {
    mu <- mean(edx$rating)
    return(rep(mu, nrow(newdata)))
    }
```


### Movie Effect Only
As seen in \greenref{data_movie_effect}, there is an effect of 
individual movies on the rating. In terms of the model,
\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
where $b_i$ is the effect (or bias) of movie $i$ across all users,
and the residual values is $\varepsilon_{u,i}$. As shown in 
\greenref{method_regularisation}, the estimator for the movie effect
is given (for the special case of $\lambda=0$) by
\[
\hat b_i = \frac{1}{n_i} \sum_{u=1}^{n_i} \left( y_{u,i} - \hat\mu \right).
\]

This estimate is implemented by the following code with calculates the
average rating $\hat\mu$ on the training set and then calculates for each
movie $i$ the $\hat b_i$ according to the above formula. Finally, the
estimates $\hat b_i$ are used to compute $\hat y_{u,i} = \hat\mu + \hat b_i$.

```{r estimator movie effect}
predict_movieb <- function(newdata) {
    # calculate mean $\mu$ of overall `edx` training set
    mu <- mean(edx$rating)

    # estimate $b_i = y_{u,i} - \mu$ for each movie $i$.
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = mean(rating - mu))

    # look up $b_i$ for each movie to predict $\hat{y}_{u,i} = \mu + b_i$
    pred_ratings <- newdata %>%
        left_join(b_i_tbl, by = "movieId") %>%
        mutate(y_hat = mu + b_i) %>% .$y_hat
    return(pred_ratings)
    }

```


### User Effect Only
We observed in \greenref{data_user_effect} an effect of users on the 
rating. Analogously to the movie effect, the user effect is expressed through
the model
\[
Y_{u,i} = \mu + b_u + \varepsilon_{u,i} 
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
The user effect $b_u$ is estimated by the equation following 
\greenref{method_regularisation}:
\[
b_u = \frac{1}{J_u} \sum_{i=1}^{J_u} \left( y_{u,i} - \hat\mu \right),
\]
where $J_u$ is the number of ratings awared by user $u$ to all movies. The 
code for the estimation of the user effect is implemented in the same way as
for the movie effect:

```{r estimator user effect}
predict_userb <- function(newdata) {
    # calculate mean $\mu$ of overall `edx` training set
    mu <- mean(edx$rating)
    
    # estimate $\hat b_u = avg(y_{u,i} - \mu)$ for each movie $i$.
    b_u_tbl <- edx %>%
        group_by(userId) %>%
        summarise(b_u = mean(rating - mu))
    
    # look up $b_u$ for each movie to predict $\hat{y}_{u,i} = \mu + \hat b_u$
    pred_ratings <- newdata %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(y_hat = mu + b_u) %>% .$y_hat
    return(pred_ratings)
    }
```


### Combined Movie and User Effect
Both effects from the previous section can be simply combined in the model
\[
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i).
\]

First, the estimate for $b_i$ is calculate in the same way as for the movie
effect only:
\[
\hat b_i = \frac{1}{n_i} \sum_{u=1}^{n_i} \left( y_{u,i} - \hat\mu \right).
\]

Then, the estimate for the user effect $b_u$ is calculated as the average
once the movie effect is already stripped out:
\[
\hat b_u = \frac{1}{J_u} \sum_{i=1}^{J_u} 
\left( y_{u,i} - \hat\mu - \hat b_i\right).
\]

These set of estimates $\hat \mu$, $\hat b_i$ and $\hat b_u$ are then used
to predict the rating $\hat y_{u,i} = \hat \mu + \hat b_i + \hat b_u$. This
successive estimation is implemented in the following code:

```{r estimator combined movie and user effect}
predict_movieuserb <- function(newdata) {
    # calculate mean $\mu$ of overall `edx` training set
    mu <- mean(edx$rating)
    
    # estimate $\hat b_i = avg(y_{u,i} - \mu)$ for each movie $i$.
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = mean(rating - mu))
    
    # estimate $\hat b_u = avg(y_{u,i} - \mu - \hat b_i)$
    b_u_tbl <- edx %>% 
        left_join(b_i_tbl, by="movieId") %>%
        group_by(userId) %>%
        summarise(b_u = mean(rating - b_i - mu))

    # apply to new data to calculate 
    # $\hat \y_{u,i} = \hat\mu + \hat b_i + \hat b_u$
    pred_ratings <- newdata %>%
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(y_hat = mu + b_i + b_u) %>% .$y_hat
    
    return(pred_ratings)
    }

```


### Regularised Movie and User Effect {#model_reg_movie_user}
There are still the same movie and user effects in the model
\[
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i).
\]

However, the estimate is given by regularisation with the parameter 
$\lambda>0$. In the boundary case of $\lambda = 0$, the estimate is the simple
average. The larger $\lambda$ becomes, the more ratings that only occur a
few times are penalised. The estimates for $\hat\mu$, $\hat b_i(\lambda)$ and 
$\hat b_u(\lambda)$ are calculated successively:

\[
\hat \mu = \frac{1}{N} \sum_{u,i} y_{u,i}.
\]
\[
\hat b_i(\lambda) = \frac{1}{\lambda + n_i} 
\sum_{u=1}^{n_i} \left( y_{u,i} - \hat\mu \right) 
\qquad (i=1,\ldots,J). \\
\]
\[
\hat b_u(\lambda) = \frac{1}{\lambda +J_U} \sum_{i=1}^{J_u} 
\left( y_{u,i} - \hat\mu - \hat b_i\right)
\qquad \text{for all } u.
\]

In addition to simply combining the effect, the parameter $\lambda$ is tuned
with k-fold cross-validation as explained at the beginning of 
\greenref{methods_modelling} in these steps:

1. Repeat for $l = 1, 2, \ldots, k = 25$:
    a) Estimate parameters $\hat\mu$, $\hat b_i(\lambda)$ and 
    $\hat b_u(\lambda)$ on the $k$-th training set.
    b) Calculate estimated ratings $\hat y_{u,i}(\lambda)$ on the $k$-th
    validation set.
    c) Use estimated ratings to compute $RMSE_l(\lambda)$ on the $k$-th 
    validation set.
2. Estimate the RMSE as 
$\hat{RMSE}(\lambda) = \frac{1}{k} \sum_{l=1}^k RMSE_l(\lambda)$.

Then repeat steps 1 through 2 for several values of $\lambda$ to find the
$\lambda$ that minimises the (estimated) $RMSE$. Note that the final test set
(which is confusingly named `validation`) is **not** used in the calculation to
minimise the RMSE.

Steps 1 and 2 of this algorithm are implemented in the following code:
```{r reg movie user cross validation}
#' Calculate RMSE using k-fold cross validation
#' for regularised movie-user effect with regularisation parameter lambda
#' 
#' @param data Complete training data, to be split into (sub-)training
#'        and validation data.
#' @param ind_list List of $k$ indices of *validation* set.
#' @param lambda Regularisation parameter $\lambda$. 
#' @return Vector of RMSEs of length $k$
RMSE_movieuser_kfold <- function(data, ind_list, lambda) {
    n <- length(ind_list)
    # iterate over indices
    rmse_v <- sapply(1:n, function (listIdx) {
        # split data int training and validation set
        train_set <- data[-ind_list[[listIdx]], ]
        validation_set <- data[ind_list[[listIdx]], ]
        # use training set for regularised movie + user effects
        mu <- mean(train_set$rating)
        b_i_tbl <- train_set %>%
            group_by(movieId) %>%
            summarise(b_i = sum(rating - mu)/(n()+lambda))
        b_u_tbl <- train_set %>% 
            left_join(b_i_tbl, by="movieId") %>%
            group_by(userId) %>%
            summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))
        # modify test set so all movies and users from training
        # set are contained in it
        validation_set <- validation_set %>%
            semi_join(train_set, by = "movieId") %>%
            semi_join(train_set, by = "userId")
        # if nothing left, can already return NA
        if(length(validation_set) == 0) {
            return(NA)
        } else {
            # otherwise estimate ratings as \mu + b_i + b_u
            ratings_hat <- 
                validation_set %>% 
                left_join(b_i_tbl, by = "movieId") %>%
                left_join(b_u_tbl, by = "userId") %>%
                mutate(pred = mu + b_i + b_u) %>%
                .$pred
            #  and finally, calculate RMSE
            return(RMSE(ratings_hat, validation_set$rating))
            }
        })
    return(rmse_v)
    }
```

Then the RMSE estimation is repeated for several values of 
$\lambda = 0, 0.25, \ldots, 10.0$:

```{r reg movie user tuning, eval = FALSE}
# define sequence of $\lambda = 0, 0.25, \ldots, 10.0$
lambdas <- seq(0, 10, 0.25)
# calculate RMSEs for all these values of $\lambda$
RMSE_data_mur <- map_df(lambdas, function(lambda) {
    # calculate vector of RMSEs
    rmse_vec <- RMSE_movieuser_kfold(edx, index_list, lambda)
    # strip out the NA values
    rmse_vec <- na.omit(rmse_vec)
    # calculate mean and standard deviation of RMSEs
    list(RMSE_avg = mean(rmse_vec),
         RMSE_sd = sd(rmse_vec))
    })

# add lambda as first columns
RMSE_data_mur <- cbind(data.frame(lambda = lambdas), 
                       RMSE_data_mur)

# look up lambda for which the RMSE is minimal
lambda_mur_opt <- lambdas[which.min(RMSE_data_mur$RMSE_avg)]
RMSE_lambda_opt <- RMSE_data_mur$RMSE_avg[which.min(RMSE_data_mur$RMSE_avg)]
```

Note that regularisation RMSE data are loaded from the file
`RMSE_data_mur.Rdata` generated by the R code that accompanies this report, as
re-running the tuning takes several hours.

```{r load reg movie user RMSE, include = FALSE}
load(file = "data//RMSE_data_mur.Rdata")
```

Plotting the RMSE for these values of $\lambda$ shows that the optimal
values is $\lambda_{mur} =$  `r lambda_mur_opt`. 

```{r reg movie user plot}
RMSE_data_mur %>%
    ggplot(aes(x = lambda, y = RMSE_avg)) +
    geom_point() +
    geom_vline(xintercept = lambda_mur_opt, linetype = "dashed") +
    labs(y = "RMSE (with bootstrapped error)", 
         title = "Tuning of movie/user lambda for regularisation")
```

Finally, since the optimal values of $\lambda$ has been determined, the
prediction function is implemented:

```{r estimator reg movie user}
#' Prediction function for regularised movie/user effect
#' 
#' @param newdata Data for which to predict the ratings.
#' @param lambda Regularisation parameter $\lambda$.
#' @return Predicted ratings
predict_regmovieuser <- function(newdata, lambda) {
    # overall mean on training data
    mu <- mean(edx$rating)
    # tabulate movie effects "b_i" with regularisation
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = sum(rating - mu)/(n()+lambda))
    # tabulate user effects "b_u" with regularisation
    b_u_tbl <- edx %>% 
        left_join(b_i_tbl, by="movieId") %>%
        group_by(userId) %>%
        summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))
    # calculate rating prediction by looking up "b_i" and "b_u"
    # from the tables just created and compute on the NEW data
    # \mu + b_i + b_u
    pred_ratings <- 
        newdata %>% 
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(pred = mu + b_i + b_u) %>%
        .$pred
    return(pred_ratings)
    }
```


### Additional Time Effect
After movie and user effect had been stripped out, an additional time effect
was observed in [Section 3.4.4](data_time_effect). The model then becomes
\[
Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
where $d_{u,i}$ is the date rounded to weeks of the rating and $f$ is a 
smooth function. Since the plotting already used LOESS, the same method will
be used here, see also \greenref{method_loess} 
The model information for the `gamLoess` method 

```{r}
modelLookup("gamLoess")
```

says that `span` and `degree` can be tuned. The degree of the local regression
is fine to leave at 1 (linear), and the span will be varied. The tuning of the
`span` parameters is achieved with the `train` function of the `caret` 
package, using the values for $d_{u,i}$ calculated in 
\greenref{data_time_effect_res}. Note that the $d_{u,i}$ values are
the residual time effects once regularised movie and user effect have already
been taking into account

```{r train loess, eval = FALSE}
train_loess <- train(d_ui ~ rating_wk, data = d_ui_tbl, 
                     method = "gamLoess",
                     tuneGrid = data.frame(degree = 1,
                                           span = seq(0.10, 0.60, 0.05)))
```

Note that since this tuning takes some time, the results are stored for this
report and loaded from the file

```{r load time effect model, include = FALSE}
load(file = "data//train_loess.Rdata")
```

The optimal span parameters is $span_opt =$  `r train_loess$bestTune$span`
as can be seen from the plot of the RMSE against this parameter:

```{r time effect plot}
# plot and look up best span parameter
ggplot(train_loess, highlight = TRUE)
span_opt <- train_loess$bestTune$span
```

The fit of the smoothing loess function through the residual time effect is
illustrated in the following plot.

```{r plot residual time effect, warning = FALSE}
# plot fit of best model to time effect
d_ui_tbl %>%
    mutate(d_ui_hat = predict(train_loess, newdata = .)) %>%
    ggplot(aes(x = as.numeric(rating_wk))) +
    geom_point(aes(y = d_ui), alpha = 0.5) +
    geom_line(aes(y = d_ui_hat), col = "blue", size = 2) +
    ylim(c(-0.1, 0.1)) +
    labs(title = "Additional time effect fitted with LOESS",
         x = "Weeks since 1 Sep 1995",
         y = "Rating Effect")
```

Finally, the model for the prediction of the residual time effect is extracted
from the training object. This object will later be used for the final
prediction of the ratings on the `validation` data set.

```{r predictor additional time effect}
# fit model for prediction of RESIDUAL time effect
fit_loess <- train_loess$finalModel

# prediction function for regularised movie and user effect plus
# additional time effect

#' Prediction function for regularised movie/user and time effect
#' 
#' @param newdata Data for which to predict the ratings.
#' @param lambda Regularisation parameter $\lambda$ for movie and user effects.
#' @param d_ui_loess LOESS model object to predict residual time effect
#'        based on week in which rating was awared.
#' @return Predicted ratings
predict_rmu_t <- function(newdata, lambda, d_ui_loess) {
    # overall mean on training data
    mu <- mean(edx$rating)
    # tabulate movie effects "b_i" with regularisation and given lambda
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = sum(rating - mu)/(n()+lambda))
    # tabulate user effects "b_u" with regularisation and given lambda
    b_u_tbl <- edx %>% 
        left_join(b_i_tbl, by="movieId") %>%
        group_by(userId) %>%
        summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))

    # calculate rating prediction by looking up "b_i" and "b_u",
    # and taking the model $f(d_ui)$ provided by d_ui_loess
    # from the tables just created and compute on the NEW data
    # $\mu + b_i + b_u + f(d_ui)$
    pred_ratings <- 
        newdata %>% 
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
        mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
        mutate(f_d_ui = predict(d_ui_loess, newdata = .)) %>%
        mutate(pred = mu + b_i + b_u + f_d_ui) %>%
        .$pred
    return(pred_ratings)
    }
```


### Additional Genre Effect
As established in \greenref{data_genre_effect}, genres have an impact
on the rating. A model that contains the movie and user effect in addition to
the genre effect is of the following form:
\[
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K \delta_{i,k} \beta_k + 
\varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
where $\mu$ is the overall rating average, $b_i$ the movie effect of movie $i$,
$b_u$ the user effect of user $u$ and $\beta_k$ the effect of genre $k$. The
$delta_{i,k}$ is an indicator function
\[
\delta_{i,k} = \begin{cases}
1, & \text{if movie } i \text{ has genre }k.\\
0, & \text{otherwise.} 
\end{cases}
\]

The estimation of all these effects is done successively, as detailed in
\greenref{model_reg_movie_user}, with the optimal 
regularisation parameter $\lambda_{mur} =$  `r lambda_mur_opt` for $b_i$ and
$b_u$. In addition, the genre effects $\beta_k$ are estimated as with a separate regularisation parameter $\lambda_g$ as
\[
\hat{\beta}_k = \frac{1}{\lambda_g +J_U} \sum_{i=1}^{J_u} 
\delta_{i,k}\left( y_{u,i} - \hat\mu - \hat b_i - \hat b_u\right)
\]

The $k$-fold cross validation follows the same algorithm described in 
\greenref{model_reg_movie_user}:

* Estimate $\hat{RMSE}(\lambda_g)$ for $lambda_g = 0, 0.25, \ldots, 10.0$ in
k-fold cross-validation with $k = 25$.
* Select the $\lambda_g$ for which the $\hat{RMSE}(\lambda_g)$ is minimal.

The algorithm is implemented by the following code:
```{r reg genre cross validation and tuning, eval = FALSE}
#' Calculate RMSE using k-fold cross validation
#' for regularised genre effect with regularisation parameter lambda
#' based on already regularised movie-user effect
#' 
#' @param data Complete training data, to be split into (sub-)training
#'        and validation data.
#' @param ind_list List of $k$ indices of *validation* set.
#' @param lambda_mur Regularisation parameter $\lamdba$ for the regularised
#'        movie and user effects.
#' @param lambda_g Regularisation parameter $\lambda$ for the genre effect.
#' @return Vector of RMSEs of length $k$
RMSE_rmu_g_kfold <- function(data, ind_list, lambda_mur, lambda_g) {
    k <- length(ind_list)
    # iterate over indices
    rmse_v <- sapply(1:k, function (listIdx) {
        # split data int training and validation set
        train_set <- data[-ind_list[[listIdx]], ]
        validation_set <- data[ind_list[[listIdx]], ]
        # use training set for regularised movie + user effects
        mu <- mean(train_set$rating)
        b_i_tbl <- train_set %>%
            group_by(movieId) %>%
            summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
        b_u_tbl <- train_set %>% 
            left_join(b_i_tbl, by="movieId") %>%
            group_by(userId) %>%
            summarise(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
        # also add on genre effect
        g_ik_tbl <- train_set %>%
            left_join(b_i_tbl, by = "movieId") %>%
            left_join(b_u_tbl, by = "userId") %>%
            group_by(genres) %>%
            summarise(g_ik = mean(rating - mu - b_i - b_u)/(n()+lambda_g))

        # modify test set so all moviess, users and genres from training
        # set are contained in it
        validation_set <- validation_set %>%
            semi_join(train_set, by = "movieId") %>%
            semi_join(train_set, by = "userId") %>%
            semi_join(train_set, by = "genres")
        
        # if nothing left, can already return NA
        if(length(validation_set) == 0) {
            return(NA)
        } else {
            # otherwise estimate ratings as \mu + b_i + b_u + \sum_k g_ik
            ratings_hat <- 
                validation_set %>% 
                left_join(b_i_tbl, by = "movieId") %>%
                left_join(b_u_tbl, by = "userId") %>%
                left_join(g_ik_tbl, by = "genres") %>%
                mutate(pred = mu + b_i + b_u + g_ik) %>%
                .$pred
            #  and finally, calculate RMSE
            return(RMSE(ratings_hat, validation_set$rating))
            }
        })
    return(rmse_v)
    }

# apply RMSE estimation for a list of lambdas
lambdas <- seq(0, 10, 0.25)
RMSE_data_g <- map_df(lambdas, function(lambda) {
    # calculate vector of RMSEs
    rmse_vec <- RMSE_rmu_g_kfold(edx, index_list, lambda_mur_opt, 
                                 lambda)
    # strip out the NA values
    rmse_vec <- na.omit(rmse_vec)
    # calculate mean and standard deviation of RMSEs
    list(RMSE_avg = mean(rmse_vec),
         RMSE_sd = sd(rmse_vec))
    })

# add lambda as first column
RMSE_data_g <- cbind(data.frame(lambda = lambdas), 
                     RMSE_data_g)
```

Since this tuning runs several hours, the results are saved to the file
`RMSE_data_g.Rdata` and loaded from there for this report to plot the estimated
RMSE against $\lambda_g$:

```{r reg genre plot}
load(file = "data//RMSE_data_g.Rdata")
RMSE_data_g %>%
    ggplot(aes(x = lambda, y = RMSE_avg)) +
    geom_point() +
    labs(x = "Regularisation parameter lambda_g",
         y = "RMSE (with bootstrapped error)", 
         title = "Tuning of genre lambda for regularisation")

# and select optimal lambda
lambda_g_opt <- RMSE_data_g$lambda[which.min(RMSE_data_g$RMSE_avg)]
```

The plot reveals that the optimal regularisation parameter is $\lambda_{g,
opt}=$ `lambda_g_opt`, proving that no regularisation was necessary for this 
particular set of training data `edx`, but this may not always be the case.

Finally, the prediction of the model with regularised movie, user and genre
effect is implemented by the following function which takes two different
regularisation parameters, $\lambda_{mur}$ for the movie and user effects and
$\lambda_g$ for the genre effect.

```{r estimator reg movie user genre}
#' Prediction function for regularised movie/user and genre effect
#' 
#' @param newdata Data for which to predict the ratings.
#' @param lambda_mur Regularisation parameter $\lambda_{mur}$ for movie and user 
#'        effects.
#' @param lamdba_g Regularisation parameter $\lambda_g$ for genre effect.
#' @return Predicted ratings
predict_rmu_g <- function(newdata, lamdba_mur, lambda_g) {
    # use total training set to first calculate overall average rating $\mu$
    mu <- mean(edx$rating)
    
    # Then construct table of regularised movie effects
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
    
    # Then tabulate the regularised user effects
    b_u_tbl <- edx %>% 
        left_join(b_i_tbl, by="movieId") %>%
        group_by(userId) %>%
        summarise(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))

    # Last, tabulate the regularised genre effect
    g_ik_tbl <- edx %>%
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        group_by(genres) %>%
        summarise(g_ik = mean(rating - mu - b_i - b_u)/(n()+lambda_g))
    
    # With that, look up all three effect (movie, user, genre) to compute 
    # $\hat{y}_{u,i} = \mu + b_i + b_u + #\sum_{k=1}^k \delta_{i,k}\beta_k$
    ratings_hat <- 
        newdata %>% 
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        left_join(g_ik_tbl, by = "genres") %>%
        mutate(pred = mu + b_i + b_u + g_ik) %>%
        .$pred
    
    return(ratings_hat)
    }
```


# Results {#results}
The RMSEs are estimated for each of the seven models presented in 
\greenref{methods_modelling} and collected in a table for comparison. All
values are computed to the full seven digits to show also marginal differences 
in performance. The code follows these steps for each method:

1. Calculate estimate according to the method on the `validation` data set.
2. Compute RMSE for this estimate.
3. Store results in table `rmse_results`.

```{r result calculation}
# 1 Constant Value
ratings_hat_const <- predict_const(validation)
rmse_const <- RMSE_rating(ratings_hat_const, validation$rating)
# create table with first row of results
rmse_results <- tibble(Method = "Simple Average",
                       RMSE = rmse_const)

# 2 Movie Effect Only
ratings_hat_movieb <- predict_movieb(validation)
rmse_movieb <- RMSE_rating(ratings_hat_movieb, validation$rating)
rmse_results <- rbind(rmse_results,
                      tibble(Method = "Movie effect",
                             RMSE = rmse_movieb))

# 3 User Effect Only
ratings_hat_userb <- predict_userb(validation)
rmse_userb <- RMSE_rating(ratings_hat_userb, validation$rating)
rmse_results <- rbind(rmse_results,
                     tibble(Method = "User effect",
                            RMSE = rmse_userb))

# 4 Combined Movie and User Effects
ratings_hat_movieuserb <- predict_movieuserb(validation)
rmse_movieuserb <- RMSE_rating(ratings_hat_movieuserb, validation$rating)
rmse_results <- rbind(rmse_results,
                      tibble(Method = "Combined Movie and User effect",
                             RMSE = rmse_movieuserb))

# 5 Regularised Movie and User Effect
ratings_hat_regmovieuser <- predict_regmovieuser(validation, lambda_mur_opt)
rmse_regmovieuser <- RMSE_rating(ratings_hat_regmovieuser, validation$rating)
rmse_results <- rbind(rmse_results,
                      tibble(Method = "Regularised Movie and User effect",
                             RMSE = rmse_regmovieuser))

# 6 Regularised Movie and User Effect, Additional Time Effect
ratings_hat_rmu_t <- predict_rmu_t(validation, lambda_mur_opt, fit_loess)
rmse_rmu_t <- RMSE_rating(ratings_hat_rmu_t, validation$rating)
rmse_results <-
    rbind(rmse_results,
          tibble(Method = "Reg. Movie and User effect, add. time effect",
                 RMSE = rmse_rmu_t))

# 7 Regularised Movie and User Effect, Additional Genre Effect
ratings_hat_rmu_g <- predict_rmu_g(validation, lambda_mur_opt, lambda_g_opt)
rmse_rmu_g <- RMSE_rating(ratings_hat_rmu_g, validation$rating)
rmse_results <-
    rbind(rmse_results,
          tibble(Method = "Reg. Movie, User and Genre effect",
                 RMSE = rmse_rmu_g))

# output table of results
knitr::kable(rmse_results)
```

The mathematical formulation of the model is shown in the following
overview:

\begin{table}[htbp] \centering
\begin{tabularx}{\textwidth}{| X | X |}
    \hline
    Method  & Model Formula \\ 
    \hline \hline
    Simple Average  & $Y_{u,i} = \mu + \varepsilon_{u,i}$ \\ \hline
    Movie effect    & $Y_{u,i} = \mu + b_i + \varepsilon_{u,i}$ \\ \hline
    User effect     & $Y_{u,i} = \mu + b_u + \varepsilon_{u,i}$ \\ \hline
    Combined Movie and User effect & 
        $Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}$ \\ \hline
    Regularised Movie and User effect & 
        $Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}$ \\ \hline
    Reg. Movie and User effect, add. time effect & 
        $Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}$\\ \hline
    Reg. Movie, User and Genre effect & 
        $Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K \delta_{i,k} \beta_k + 
        \varepsilon_{u,i}$ \\ \hline
\end{tabularx}
\end{table}

As expected, the simple average performs worst as it contains no specific
information from the movies or users. The incorporation of the movie or the user
effects each provide an improvement. The movie effect alone performs better 
than the user effect as there is higher variability in the user effect, as 
observed in [Section 3.4.3]{#data_user_effect}.

Combining the movie and user effects has a largest incremental effect. That
the regularisation only leads to a marginal improvement is due to the fact that
the vast majority of movies have 10 or more ratings (see also \greenref{data_ratings_by_movie_figure}).

Using the regularised movie and user effect model as building block to add
more information it can be observed that:

* Adding the time of the rating actually produces a worse performance than 
leaving it out.
* Adding the genre of the movie leads to only a slight improvment in
performance, but does result in the best-performing model.

# Conclusion {#conclusion}
With elementary methods such as regularised average and local regression
(LOESS), ratings can be forecast with an accuracy of 0.8648 stars (as measured
by RMSE) based on just over 1% of the total combinations of movie and user
ratings using only the elementary information of movie and user effects. The
genre carries only little additional information, while adding the  time of the
rating even slightly deteriorates the performance.

So the overall best model among those tested is the regularised movie, user
and genre effect, with the same regularisation parameter for movie and user
and a different regularisation parameter for the genre.

It may be possible to boost the performance further by 

* Separating the genres that have been used cumulatively into their 
constituents.
* Performing a principle component analysis (PCA) of the matrix of ratings
arranged by user for rows and movie for columns and taking the first 
components as additional factors in the model. This may become unwieldy, though
since such a matrix has about 750 million entries.

\newpage
\fontsize{14}{16}\selectfont{\textbf{Appendix A - Code of Figures}}

\hypertarget{appendix_figures} To enhance readibility of the report, the code
for most figures in [Section 3.4](methods_data_exp_vis) is shown in this
Appendix.

\fontsize{12}{14}\selectfont{}
\textbf{Figures in Section 3.4.1 - General Data Distribution}

\hypertarget{code_sparsity_figure}
Code for *data sparsity* figure shown [\greenuline{here}](#data_sparsity_figure):

```{r code data matrix of subset, eval = FALSE}
edx_1000 %>%
    mutate(rating = 1) %>%
    select(movieId, userId, rating) %>%
    spread(movieId, rating) %>% 
    column_to_rownames(var = "userId") %>% 
    as.matrix() %>% t() %>%
    image(x = 1:n_movies_sub, y = 1:n_users_sub, z = ., 
          xlab = "Movies", ylab = "Users",
          col = grey.colors(n = 1, start = 0, end = 1))
```

\hypertarget{code_ratings_by_movie_figure}
Code for *ratings counts by movie* figure shown 
[\greenuline{here}](#data_ratings_by_movie_figure):
```{r code rating by movie, eval = FALSE}
edx %>%
    group_by(movieId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black") +
    labs(title = "Rating counts by movie", 
         x = "Number of Ratings (log scale)")
```

\hypertarget{code_ratings_by_user_figure}
Code for *rating counts by user* figure shown 
[\greenuline{here}](#data_ratings_by_user_figure):
```{r code rating by user, eval = FALSE}
edx %>%
    group_by(userId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 40, col = "black") +
    labs(title = "Rating counts by user", 
         x = "Number of Ratings (log scale)")
```

\hypertarget{code_ratings_distribution_figure}
Code for *distribution of ratings - total* histogram shown [\greenuline{here}](#data_ratings_distribution_figure):
```{r code rating histogram, eval = FALSE}
edx %>%
    count(rating) %>%
    ggplot(aes(x = factor(rating), y = n)) +
    geom_bar(stat = "identity", width = 1, col = "black")+
    labs(title = "Distribution of ratings - total",
         x = "Rating", y = "Count") 
```

\textbf{Figure in Section 3.4.2 - Movie Effect}

\hypertarget{code_dist_ratings_by_movie}
Code for *movie effect* chart shown [\greenuline{here}](#data_movie_effect):
```{r code dist rating by movie, eval = FALSE}
# distribution of ratings by movie
movie_avgs <- edx %>%
    group_by(movieId) %>%
    summarise(movie_avg = mean(rating),
              movie_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_bymovie = n()) %>%
    arrange(movie_avg) %>%
    mutate(row = row_number(movie_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_avg)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by movie")
p2 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_sd)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```

\textbf{Figure in Section 3.4.3 - User Effect}

\hypertarget{code_dist_ratings_by_user}
Code for *user effect* chart shown [\greenuline{here}](#data_user_effect):
```{r code dist rating by user, eval = FALSE}
# distribution of ratings by user - in appendix
user_avgs <- edx %>%
    group_by(userId) %>%
    summarise(user_avg = mean(rating),
              user_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_byuser = n()) %>%
    arrange(user_avg) %>%
    mutate(row = row_number(user_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- user_avgs %>%
    ggplot(aes(x = row, y = user_avg)) +
    geom_point() +
    labs(x = "User (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by user")
p2 <- user_avgs %>%
    ggplot(aes(x = row, y = user_sd)) +
    geom_point() +
    labs(x = "User (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```

\textbf{Figures in Section 3.4.4 - Time of Rating}

\hypertarget{code_ratings_by_time}
Code for *time effect* chart shown [\greenuline{here}](#data_time_effect_total):
```{r code rating by time, eval = FALSE}
# tabulate rating by time
 edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    group_by(ratingdate_wk) %>%
    summarize(rating_wk = mean(rating))
    ggplot(aes(x = ratingdate_wk, y = rating_wk)) +
    geom_point() +
    geom_smooth()
```

\hypertarget{code_ratings_by_time_res}
Code for *residual time effect* once movie and user effect are stripped out,
in chart shown [\greenuline{here}](#data_time_effect_res):
```{r code residual time effect, eval = FALSE}
min_date = min(edx$ratingdate)
lambda_mur <- 5.0
# Calculate regularised movie user and time effect, see 
mu <- mean(edx$rating)
# Movie effect
b_i_tbl <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
# User effect
b_u_tbl <- edx %>% 
    left_join(b_i_tbl, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
# Residual time effect
d_ui_tbl <- edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
    left_join(b_i_tbl, by = "movieId") %>%
    left_join(b_u_tbl, by = "userId") %>%
    group_by(rating_wk) %>%
    summarise(d_ui = mean(rating - mu - b_i - b_u))

# plot the overall time effect once movie and user effect are accounted for
d_ui_tbl %>%
    ggplot(aes(x = as.numeric(rating_wk), y = d_ui)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Residual time effect of ratings",
         x = paste0("Weeks since ",format(min_date, "%d %b %Y")),
         y = "Residual rating effect (stars)")
```


\textbf{Figures in Section 3.4.5 - Genre Effect}

\hypertarget{code_nratings_by_genre}
The code for the first chart in \greenref{data_genre_effect} is:
```{r code genre number of ratings, eval = FALSE}
edx %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              nratings = n()) %>%
    arrange(nratings) %>%
    ggplot(aes(x = nratings)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black")
```

\hypertarget{code_ratings_by_genre}
The code for the second chart is:
```{r code ratings by genre, eval = FALSE}
# First chart - average ratings by genres, in ascending order
p1 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_avg)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "Ratings by genres (with at least 1000 ratings)",
         x = "Genre (sorted by average rating)",
         y = "Average Rating")

# Second chart - standard dev of ratings by genre, genres need to
# be in same order as in first chart
p2 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_sd)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "",
         x = "Genre (sorted by average rating)",
         y = "Rating Standard Deviation")

grid.arrange(p1, p2, nrow = 2)
```
