---
title: "MovieLens Project"
author: "Dr Daniel Mayenberger"
date: "May 2020"
header-includes:
   - \usepackage{hyperref}
   - \usepackage[usenames,dvipsnames]{xcolor}
   - \usepackage{amsmath}
output: 
    pdf_document:
        number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries and load data, include = FALSE}
# libraries
library(tidyverse)
library(lubridate)
library(caret)
library(gam)
library(ggplot2)
library(gridExtra)
# Set to TRUE for final version, is kept at FALSE for quicker compiling
# of drafts
FINAL_VERSION = FALSE

# load data from local repository
if(FINAL_VERSION) {
    source("loadLocalData.R")
} else {
    source("loadLocalSampleData.R")
    edx <- edx_1000
    validation <- validation_1000
    }
```

\hypersetup {
    linkcolor=green
    urlcolor=cyan
    }

# Executive Summary
Given over 10,000 movies and about 70,000 users that have rated some of these
movies on a rating scale from zero to five stars, the goal is to predict future
ratings based on past ratings. The challenge is to do this for the over 700
millions different movie/user combinations  based on only 1.2% (9 millions) of
such combinations used for training the algorithm.

The available data of 10 million ratings are split into 9 million ratings to
train different algorithms and 1 million ratings to evaluate the performance
of these methods. Performance is the root-mean squared error (RMSE) of ratings.

Based on the RMSE, the model that performs best with an RMSE of [TBD] is
\[
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K \delta_{i,k} \beta_k + 
\varepsilon_{u,i},
\]
where $Y_{u,i}$ is the rating by user $u$ of movie $i$, and $\delta_{i,k}$ is
an indicator which is $1$ if the movie $i$ is of genre $k$ and zero otherwise.

# Introduction
The movielens project makes available millions of a movie ratings provided by
anonymised users.

From the movielens website information about the data [here](http://files.grouplens.org/datasets/movielens/ml-10m-README.html)
the following further information is provided: 

* The data set contains just over 10 million ratings by users of the online
movie recommender service MovieLens.
* Users were selected at random for inclusion. All users selected had rated at
least 20 movies. Each user is represented by an id, and no other information is
provided.

Denoting the rating by any of these users $u$ of a certain movie $i$, the task
is to predict ratings $Y_{u,i}$ of any such user/movie combination $(u,i)$. The
challenge is that there are overall 71,567 users and 10,681 movies, yielding
$71,567 \times 10,681 = 764,407,127$ while only 10 million data points, so only
about 1.3% $(=10/764.4)$ such data are available in totality and 10% (1 million)
of the data must be held out as test set, leaving 1.2% $(=9/764.4)$ of data to
train a rating prediction algorithm.

To do so, the data are examined and modelled in [**Section 3**](#methods) which is
further broken down into

* [Section 3.1](#methods_techniques) to elaborate on the techniques used, in 
particular in the modelling [Section 3.5](#methods_modelling).
* [Section 3.2](#methods_data_structure) to provide an overview of the basic
structure of the rating data.
* [Section 3.3](#methods_data_cleaning) to perform data cleaning.
* [Section 3.4](#methods_data_exp_vis) to visualise the most important 
properties of the ratings with respect to individual movies, their genres
or individual users. These properties then inspire the modelling methods
in the subseqeuent section.
* [Section 3.5](#methods_modelling) presents the models based on the most
salient data properties and calibrates any free modelling parameters.

The results of all models are summarised in [Section 4](#results) and the
conclusions are drawn in [Section 5](#conclusion).

# Methods and Analysis {#methods}

## Process and Techniques Used {#methods_techniques}

Two modelling techniques will be described, first the regularisation in 
[Section 3.1.1](#method_regularisation) and then the local estimated scatterplot 
smoothing (LOESS) method in [Section 3.1.2](#method_loess).

### Regularisation {#method_regularisation}
The modelling techniques employed are regularised least squared estimates.
Specifically, a model of the form
\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]
for user $u$ and movie $i$ assumes that there is an overall average rating of
$\mu$ across all movies. The $b_i$ is based on the observation that some movies
are better rated than others, as will be shown in [Section
3.4.2](#data_movie_effect). 
However, as there are some movies that are rated by hundreds or more users, the
ratings of these movies are more reliable than the ratings of movies that are
only rated by a few users. To calculate such an effect, denote the number of
ratings awarded to movie $i$ by $n_i$, the overall number of ratings as $N$ and
the overall number of movies by $J$. Further, denote the overall average rating
with $\mu$.

Without any weighing of ratings for a movie by the number of ratings awarded to
it, the goal is to minimise the MSE given by
\[
MSE = \frac{1}{N} \sum_{i=1}^J \sum_{u=1}^{n_i} (y_{u,i} - \hat{y}_{u,i})^2.
\]
As for any user $u$ and movie $i$ the modelled rating is given by 
$\hat{y}_{u,i} = \mu + \hat{b}_i$, the MSE to minimise becomes
\[
MSE = \frac{1}{N} \sum_{i=1}^J \sum_{u=1}^{n_i} 
\left( y_{u,i} - (\mu + \hat{b}_i) \right)^2.
\]
The necessary condition for a minimum is $\frac{\partial MSE}{\partial b_j}=0$
for all $j=1,2, \ldots, J$ which leads to
\[
0 = -\frac{2}{N} \sum_{u=1}^{n_j} \left( y_{u,j} - (\mu + \hat{b}_j) \right)
\quad (j=1,\ldots,J).
\]
This is equivalent to
\[
\hat{b}_j = \frac{1}{n_j} \sum_{u=1}^{n_j}(y_{u,j} - \mu) \quad (j=1,\ldots,J).
\]
To penalise estimates for movies that are only rated by a few users, the
MSE to minimise then becomes
\[
MSE(\lambda) = \frac{1}{N} \left[
\sum_{i=1}^J \sum_{u=1}^{n_i} (y_{u,i} - \hat{y}_{u,i})^2 +
\lambda \sum_{i=1}^J b_i^2
\right]
\]
with a regularisation parameter $\lambda \geq 0$. Again, the necessary condition
for a minimum of MSE is $\frac{\partial MSE(\lambda)}{\partial b_j}=0$
for all $j=1,2, \ldots, J$ which implies
\[
0 = -\sum_{u=1}^{n_j} \left( y_{u,j} - (\mu + \hat{b}_j) \right) +
\lambda b_j
\quad (j=1,\ldots,J).
\]
Solving for $\hat{b}_j$ yields
\[
\hat{b}_j = \frac{1}{n_j+\lambda} 
\sum_{u=1}^{n_j}(y_{u,j} - \mu) \quad (j=1,\ldots,J).
\]
This solution is a minimum, since the second derivative (Hessian matrix) of the
MSE function is a diagonal matrix, as 
$\frac{\partial^2MSE}{\partial b_i \partial b_j} = 0$ for $i \neq j$ and
$\frac{\partial^2MSE}{\partial b_i^2} = 2\frac{n_i+\lambda}{N} > 0$. There are
only positive entries on the diagonal of this matrix, and with that it is
positively definitive. So the solution for $\hat{b}_j$ provided above is
a local minimum and as there are no other local optima (there is only one
point at which the first partial derivative vanishes), it is also a global
minimum.

### Local Estimated Scatterplot Smoothing (LOESS) {#method_loess}
Unlike standard local regression that fits a line to the whole data set,
the locally estimated scatterplot smoothing (LOESS) estimates a regression
line in a local window that is progressively moved through the data. The
parameter of LOESS is the width of this window.

## Data Structure and Loading {#methods_data_structure}
The raw data is provided in two sets, called `edx` and `validation`.

### Sample Data {#data_sample_data}
To facilitate code verification, 1,000 samples of data have been extracted from
both datasets `edx` and `validation`, called `edx_1000` and `validation_1000`
respectively. This is achieved with the following code:

```{r data sample data, eval = FALSE}
set.seed(123, sample.kind = "Rounding")
edx_1000 <- sample_n(edx, size = 1000)
validation_1000 <- sample_n(validation, size = 1000)
```

### Basic Data Structure
Both `edx` and `validation` have the same structure that can be displayed with 
the `str` function and is listed below for `edx`:

|Column name   |Type      |First values                                                                                     |
|:---------|:---------|:-----------------------------------------------------------------------------------------------|
|userId    |integer   |1 1 1                                                                                           |
|movieId   |numeric   |122 185 292                                                                                     |
|rating    |numeric   |5 5 5                                                                                           |
|timestamp |integer   |838985046 838983525 838983421                                                                   |
|title     |character |Boomerang (1992) Net, The (1995) Outbreak (1995)                                                |
|genres    |character |Comedy&#124;Romance Action&#124;Crime&#124;Thriller Action&#124;Drama&#124;Sci-Fi&#124;Thriller |


The data sets are both tidy as well - the count of `NA` values is zero for both 
sets:
```{r data tidy}
edx %>% summarise_all(~sum(is.na(.)))
validation %>% summarise_all(~sum(is.na(.)))
```

The `userId`, `movieId` , `rating` and `title` data can be used directly in
the format provided. These two columns need further processing:

1. The `timestamp` column is in raw data format of seconds since 1 January 1970 
and will be converted into a date & time format.
2. The `genres` column contains one ore more genres for each movie, separated
by a pipe (`|`) symbol, so they will be separated.

## Data Cleaning {#methods_data_cleaning}
### Conversion of Timestamp to Date
The `timestamp` column is converted to a date and time using the `as_datetime`
function for both the `edx` and the `validation` dataset and stored in 
a new column `ratingdate`. After the conversion the `timestamp` columns
are no longer required and are discarded.
```{r data conversion of date}
edx <- edx %>%
    mutate(ratingdate = as_datetime(timestamp)) %>%
    select(-timestamp)

validation <- validation %>%
    mutate(ratingdate = as_datetime(timestamp)) %>%
    select(-timestamp)
```

### Separation of Genres
[TBD once needed]

## Data Exploration and Visualisation {#methods_data_exp_vis}

### General Data Distribution Properties
The `edx` data set contains 9,000,055 data point, consisting of 10,677
distinct movies and 69,878 different users:
```{r data count}
edx %>%
    summarise(n_movies = n_distinct(movieId),
              n_users = n_distinct(userId))
```
With that, the total number of diferent movie/user combinations is
$10,677 \times 69,878 = 746,087,406$, much more than the overall 10 million
data points from the combined `edx` and `validation` sets. While it is not
feasible with the memory capacity of a personal computer to visualise all of
the over 700 million combinations, it is possible to illustrate the sparsity
of the data set on a representative sample.

```{r, include = FALSE}
n_movies_sub <- length(unique(edx_1000$movieId))
n_users_sub <- length(unique(edx_1000$userId))
```

For better readability, the code for the subsequent graphs in this Section
are displayed in \hyperlink{appendix_figures}{Appendix A}.

#### Data Sparsity {#data_sparsity_figure}
Within the sample of 1,000 ratings drawn as described in 
[Section 3.2.1](#data_sample_data) there are already `r n_movies_sub`
different movies and `r n_users_sub` different users. Plotting the
data points of this matrix shows that the data given are very sparse 
but at least evenly distributed among movies and users.

```{r data matrix of subset, echo = FALSE}
edx_1000 %>%
    mutate(rating = 1) %>%
    select(movieId, userId, rating) %>%
    spread(movieId, rating) %>% 
    column_to_rownames(var = "userId") %>% 
    as.matrix() %>% t() %>%
    image(x = 1:n_movies_sub, y = 1:n_users_sub, z = ., 
          xlab = "Movies", ylab = "Users",
          col = grey.colors(n = 1, start = 0, end = 1))
```

The code is shown \hyperlink{code_sparsity_figure}{here}.

#### Ratings by Movie {#data_ratings_by_movie_figure}

There is a wide variety of the number of ratings by movie. Some movies are
rated only up to 10 times, while others are rated thousands of times:

```{r rating by movie, echo = FALSE}
edx %>%
    group_by(movieId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black") +
    labs(title = "Rating counts by movie", 
         x = "Number of Ratings (log scale)")
```

The code is shown \hyperlink{code_ratings_by_movie_figure}{here}.

#### Ratings by User {#data_ratings_by_user_figure}

Similarly, there are users that only rate a few movies while other rate
hundreds or even thousands of movies:

```{r rating by user, echo = FALSE}
edx %>%
    group_by(userId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 40, col = "black") +
    labs(title = "Rating counts by user", 
         x = "Number of Ratings (log scale)")
```

The code is shown \hyperlink{code_ratings_by_user_figure}{here}.

#### Distribution of Ratings {#data_ratings_distribution_figure}

The ratings most awared are three and four stars. In general, whole-star
ratings are more frequently given than half-star ratings:

```{r rating histogram, echo = FALSE}
edx %>%
    count(rating) %>%
    ggplot(aes(x = factor(rating), y = n)) +
    geom_bar(stat = "identity", width = 1, col = "black")+
    labs(title = "Distribution of ratings - total",
         x = "Rating", y = "Count") 
```

The code is displayed \hyperlink{code_ratings_distribution_figure}{here}.

### Movie Effect {#data_movie_effect}
From public movie ratings such as [Rotten
Tomatoes](https://www.rottentomatoes.com/) it is known that some movies are in
general better rated than others. We group the ratings by movie ID to 
evaluate the same effect. In addition to the average rating for each movie,
the standard deviation of ratings for the same movie is visualised in parallel.

```{r dist rating by movie, echo = FALSE}
# distribution of ratings by movie
movie_avgs <- edx %>%
    group_by(movieId) %>%
    summarise(movie_avg = mean(rating),
              movie_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_bymovie = n()) %>%
    arrange(movie_avg) %>%
    mutate(row = row_number(movie_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_avg)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by movie")
p2 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_sd)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is \hyperlink{code_dist_ratings_by_movie}{here}.

So there are indeed movies that, on average:

* are rated worse than other movies, on the left side of the chart.
* are rated better than other movies, towards the right of the chart.

At the same time, there is a high variability of this *movie effect*, 
illustrated by the broad range of the standard deviation in the 
lower half of the figure. Standard deviation of ratings by movie hovers around
the value of one (star).

### User Effect {#data_user_effect}
Similar to the movie effect, different users may have the tendency to award
higher or lower ratings, compared to other users.

```{r dist rating by user, echo = FALSE}
# distribution of ratings by user
user_avgs <- edx %>%
    group_by(userId) %>%
    summarise(user_avg = mean(rating),
              user_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_byuser = n()) %>%
    arrange(user_avg) %>%
    mutate(row = row_number(user_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- user_avgs %>%
    ggplot(aes(x = row, y = user_avg)) +
    geom_point() +
    labs(x = "User (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by user")
p2 <- user_avgs %>%
    ggplot(aes(x = row, y = user_sd)) +
    geom_point() +
    labs(x = "User (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is \hyperlink{code_dist_ratings_by_user}{here}.

So similarly to the movie effect, there is a *user effect* with users that 
tend to assign movies lower rating on the left hand side of the chart and
users that rate movies more highly towards the right. In constrast to the
*movie effect* there is much higher variability of rating, consistent with
users differentiating between good and bad movies.


### Time of Rating {#data_time_effect}
Another feature that may influence the rating is the date and time at which
the rating was awarded. When averaging all ratings within a week and following
this average with a LOESS function, the following pattern emerges:

#### Overall Time Effect {#data_time_effect_total}
```{r rating by time, echo = FALSE}
edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    group_by(ratingdate_wk) %>%
    summarise(rating_wk = mean(rating)) %>%
    ggplot(aes(x = ratingdate_wk, y = rating_wk)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Rating by date",
         x = "Rating date (granularity of weeks)",
         y = "Rating")
```
  
The code for this chart is \hyperlink{code_ratings_by_time}{here}.

So there is a small effect that is worth capturing, as a smooth function
of time. 

#### Residual Time Effect {#data_time_effect_res}

Since the effect will be modelled after movie effect and user
effect are accounted for, it is also informative to plot the residual
effect.

```{r residual time effect, echo = FALSE}
min_date = min(edx$ratingdate)
lambda_mur <- 5.0
# Calculate regularised movie user and time effect, see 
mu <- mean(edx$rating)
# Movie effect
b_i_tbl <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
# User effect
b_u_tbl <- edx %>% 
    left_join(b_i_tbl, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
# Residual time effect
d_ui_tbl <- edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
    left_join(b_i_tbl, by = "movieId") %>%
    left_join(b_u_tbl, by = "userId") %>%
    group_by(rating_wk) %>%
    summarise(d_ui = mean(rating - mu - b_i - b_u))

# plot the overall time effect once movie and user effect are accounted for
d_ui_tbl %>%
    ggplot(aes(x = as.numeric(rating_wk), y = d_ui)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Residual time effect of ratings",
         x = paste0("Weeks since ",format(min_date, "%d %b %Y")),
         y = "Residual rating effect (stars)")

```

The code for this figure can be found 
\hyperlink{code_ratings_by_time_res}{here}.

In comparison to the total effect, the residual time effect is:

* much stronger in the first 2 years, after `r format(min_date, "%d %b %Y")`.
* considerable flatter after the the first 2 years.

### Genre Effect {#data_genre_effect}

As final feature, we investigate whether certain genres (or their combinations)
are rated differently from others. First, we determine whether genres have
sufficiently many ratings by examining the distribution of the number of
ratings by genre:

```{r genre number of ratings, echo = FALSE}
edx %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              nratings = n()) %>%
    arrange(nratings) %>%
    ggplot(aes(x = nratings)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black")
```

The code for this figure is shown \hyperlink{code_nratings_by_genre}{here}.

So there are definitely sufficiently many genres with 1,000 or more ratings.
In the same way as for movies and users, we group the rating by genres for
all genres with over 1,000 rating and chart their average rating and standard 
deviation.

```{r ratings by genre, echo = FALSE}
p1 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_avg)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "Ratings by genres (with at least 1000 ratings)",
         x = "Genre (sorted by average rating)",
         y = "Average Rating")

p2 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_sd)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "",
         x = "Genre (sorted by average rating)",
         y = "Rating Standard Deviation")

grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is shown \hyperlink{code_ratings_by_genre}{here}.

So the genre combination does have an effect on the rating, in a similar way
as movies and users do.

### Extrapolation Requirements Check
To predict movie ratings and calculate the final RMSE, it must be considered 
how the algorithm handles data in the `validation` data set to which it was
not trained in the `edx` set. It will be shown that there is no need for 
such extrapolation for the provided data sets.

By construction, the `validation` set only contains movies and users that 
are also in the `edx` set, with the `semi_join` statements provided in
the instructions. For verification, this code confirms that indeed no movies
or users from the `validation` set are missing from the `edx` set:

```{r}
validation %>%
    anti_join(edx, by = "movieId") %>%
    group_by(movieId, title) %>%
    summarise(n = n())

validation %>%
    anti_join(edx, by = "userId") %>%
    group_by(userId) %>%
    summarise(n = n())
```

In addition, all genres, and even more specifically, all their combinations
that are present in the `validation` set are also found in the `edx` set, 
as this code shows:
```{r}
validation %>%
    anti_join(edx, by = "genres") %>%
    group_by(genres) %>%
    summarise(n = n())
```
So with the given data set, no extrapolation to movies, users or genres needs
to be made in predicting ratings and determining model performance.

## Modelling Approach {#methods_modelling}
The training of model parameters is generally done using k-fold 
cross-validation. To do this, the `edx` data is split into a training and
a validation set $k$ times. 

Graphically, k-fold validation can be illustrated by $k$ subsequent splits of
the overall `edx` set into training sets (shown in blue) and test sets (shown in
purple).

```{r kfold crossvalidation, echo = FALSE}
n_data <- nrow(edx)
validation_split <- data.frame(
    k = seq(1, 25, 1),
    C = seq(0, 24, 1) / 25 * n_data,
    B = rep(1/25 * n_data, time = 25))
validation_split <- validation_split %>%
    mutate(A = n_data - B - C)
validation_split <- validation_split %>%
    gather(set, value, -k) 

validation_split %>%
    ggplot(aes(x = k, y = value, fill = set)) +
    scale_fill_manual(values = c("#ACE5EE", "#FAF0BE", "#ACE5EE")) +
    theme(plot.margin = margin(0,0,0,0, "cm")) +
    geom_bar(stat = "identity", position = "stack", show.legend = FALSE) +
    labs(title = "Illustration of k-fold cross validation (k = 25)",
         y = "Values")
```
\definecolor{blizzardblue}{rgb}{0.67, 0.9, 0.93}
\definecolor{blond}{rgb}{0.98, 0.94, 0.75}

For example, the parameter $\lambda$ is applied to the
\colorbox{blizzardblue}{training data} and the RMSE that results from that
particular values of $\lambda$ is evaluated in the 
\colorbox{blond}{test data}.

The **performance** of each algorithm is evaluated using the RMSE which is
implemented in the function `RMSE_rating`:

```{r RMSE function}
RMSE_rating <- function(pred_ratings, true_ratings) {
    ifelse(length(pred_ratings) > 0,
           sqrt(mean((pred_ratings - true_ratings)^2)),
           NA)
    }
```

### Constant Value

This is the simplest baseline model that assumes that the rating is the
same every time. The model is
\[
Y_{u,i} = \mu + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
The parameter $\mu$ is estimated as average across all ratings:
\[
\hat{\mu} = \frac{1}{N} \sum_{u,i} y_{u,i}.
\]

The R implementation of this model is given by a function that returns the
same value for each record of the requested data set:
```{r estimator const}
predict_const <- function(newdata) {
    mu <- mean(edx$rating)
    return(rep(mu, nrow(newdata)))
    }
```


### Movie Effect Only
As seen in [Section 3.4.2](#data_movie_effect), there is an effect of 
individual movies on the rating. In terms of the model,
\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
where $b_i$ is the effect (or bias) of movie $i$ across all users,
and the residual values is $\varepsilon_{u,i}$. As shown in 
[Section 3.1.1](# method_regularisation), the estimator for the movie effect
is given (for the special case of $\lambda=0$) by
\[
\hat b_i = \frac{1}{n_i} \sum_{u=1}^{n_i} \left( y_{u,i} - \hat\mu \right).
\]

This estimate is implemented by the following code with calculates the
average rating $\hat\mu$ on the training set and then calculates for each
movie $i$ the $\hat b_i$ according to the above formula. Finally, the
estimates $\hat b_i$ are used to compute $\hat y_{u,i} = \hat\mu + \hat b_i$.
```{r estimator movie effect}
predict_movieb <- function(newdata) {
    # calculate mean $\mu$ of overall `edx` training set
    mu <- mean(edx$rating)

    # estimate $b_i = y_{u,i} - \mu$ for each movie $i$.
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = mean(rating - mu))

    # look up $b_i$ for each movie to predict $\hat{y}_{u,i} = \mu + b_i$
    pred_ratings <- newdata %>%
        left_join(b_i_tbl, by = "movieId") %>%
        mutate(y_hat = mu + b_i) %>% .$y_hat
    return(pred_ratings)
    }

```


### User Effect Only
We observed in [Section 3.4.3](#data_user_effect) an effect of users on the 
rating. Analogously to the movie effect, the user effect is expressed through
the model
\[
Y_{u,i} = \mu + b_u + \varepsilon_{u,i} 
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
The user effect $b_u$ is estimated by the equation following 
[Section 3.1.1](# method_regularisation):
\[
b_u = \frac{1}{J_u} \sum_{i=1}^{J_u} \left( y_{u,i} - \hat\mu \right),
\]
where $J_u$ is the number of ratings awared by user $u$ to all movies. The 
code for the estimation of the user effect is implemented in the same way as
for the movie effect:

```{r estimator user effect}
predict_userb <- function(newdata) {
    # calculate mean $\mu$ of overall `edx` training set
    mu <- mean(edx$rating)
    
    # estimate $\hat b_u = avg(y_{u,i} - \mu)$ for each movie $i$.
    b_u_tbl <- edx %>%
        group_by(userId) %>%
        summarise(b_u = mean(rating - mu))
    
    # look up $b_u$ for each movie to predict $\hat{y}_{u,i} = \mu + \hat b_u$
    pred_ratings <- newdata %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(y_hat = mu + b_u) %>% .$y_hat
    return(pred_ratings)
    }
```


### Combined Movie and User Effect
Both effects from the previous section can be simply combined in the model
\[
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i).
\]

First, the estimate for $b_i$ is calculate in the same way as for the movie
effect only:
\[
\hat b_i = \frac{1}{n_i} \sum_{u=1}^{n_i} \left( y_{u,i} - \hat\mu \right).
\]

Then, the estimate for the user effect $b_u$ is calculated as the average
once the movie effect is already stripped out:
\[
\hat b_u = \frac{1}{J_u} \sum_{i=1}^{J_u} 
\left( y_{u,i} - \hat\mu - \hat b_i\right).
\]

These set of estimates $\hat \mu$, $\hat b_i$ and $\hat b_u$ are then used
to predict the rating $\hat y_{u,i} = \hat \mu + \hat b_i + \hat b_u$. This
successive estimation is implemented in the following code:

```{r estimator combined movie and user effect}
predict_movieuserb <- function(newdata) {
    # calculate mean $\mu$ of overall `edx` training set
    mu <- mean(edx$rating)
    
    # estimate $\hat b_i = avg(y_{u,i} - \mu)$ for each movie $i$.
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = mean(rating - mu))
    
    # estimate $\hat b_u = avg(y_{u,i} - \mu - \hat b_i)$
    b_u_tbl <- edx %>% 
        left_join(b_i_tbl, by="movieId") %>%
        group_by(userId) %>%
        summarise(b_u = mean(rating - b_i - mu))

    # apply to new data to calculate 
    # $\hat \y_{u,i} = \hat\mu + \hat b_i + \hat b_u$
    pred_ratings <- newdata %>%
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(y_hat = mu + b_i + b_u) %>% .$y_hat
    
    return(pred_ratings)
    }

```


### Regularised Movie and User Effect
There are still the same movie and user effects in the model
\[
Y_{u,i} = \mu + b_i + b_u + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i).
\]

However, the estimate is given by regularisation with the parameter 
$\lambda>0$. In the boundary case of $\lambda = 0$, the estimate is the simple
average. The larger $\lambda$ becomes, the more ratings that only occur a
few times are penalised. The estimates for $\hat\mu$, $\hat b_i(\lambda)$ and 
$\hat b_u(\lambda)$ are calculated successively:

\[
\hat \mu = \frac{1}{N} \sum_{u,i} y_{u,i}.
\]
\[
\hat b_i(\lambda) = \frac{1}{\lambda + n_i} 
\sum_{u=1}^{n_i} \left( y_{u,i} - \hat\mu \right) 
\qquad (i=1,\ldots,J). \\
\]
\[
\hat b_u(\lambda) = \frac{1}{\lambda +J_U} \sum_{i=1}^{J_u} 
\left( y_{u,i} - \hat\mu - \hat b_i\right)
\qquad \text{for all } u.
\]

In addition to simply combining the effect, the parameter $\lambda$ is tuned
with k-fold cross-validation as explained at the beginning of 
[Section 3.5](methods_modelling) in these steps:

1. Repeat for $l = 1, 2, \ldots, k = 25$:
    a) Estimate parameters $\hat\mu$, $\hat b_i(\lambda)$ and 
    $\hat b_u(\lambda)$ on the $k$-th training set.
    b) Calculate estimated ratings $\hat y_{u,i}(\lambda)$ on the $k$-th
    validation set.
    c) Use estimated ratings to compute $RMSE_l(\lambda)$ on the $k$-th 
    validation set.
2. Estimate the RMSE as 
$\hat{RMSE}(\lambda) = \frac{1}{k} \sum_{l=1}^k RMSE_l(\lambda)$.

Then repeat steps 1 through 2 for several values of $\lambda$ to find the
$\lambda$ that minimises the (estimated) $RMSE$. Note that the final test set
(which is confusingly named `validation`) is **not** used in the calculation to
minimise the RMSE.

Steps 1 and 2 of this algorithm are implemented in the following code:
```{r reg movie user cross validation}
#' Calculate RMSE using k-fold cross validation
#' for regularised movie-user effect with regularisation parameter lambda
#' 
#' @param data Complete training data, to be split into (sub-)training
#'        and validation data.
#' @param ind_list List of $k$ indices of *validation* set.
#' @param lambda Regularisation parameter $\lambda$. 
#' @return Vector of RMSEs of length $k$
RMSE_movieuser_kfold <- function(data, ind_list, lambda) {
    n <- length(ind_list)
    # iterate over indices
    rmse_v <- sapply(1:n, function (listIdx) {
        # split data int training and validation set
        train_set <- data[-ind_list[[listIdx]], ]
        validation_set <- data[ind_list[[listIdx]], ]
        # use training set for regularised movie + user effects
        mu <- mean(train_set$rating)
        b_i_tbl <- train_set %>%
            group_by(movieId) %>%
            summarise(b_i = sum(rating - mu)/(n()+lambda))
        b_u_tbl <- train_set %>% 
            left_join(b_i_tbl, by="movieId") %>%
            group_by(userId) %>%
            summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))
        # modify test set so all movies and users from training
        # set are contained in it
        validation_set <- validation_set %>%
            semi_join(train_set, by = "movieId") %>%
            semi_join(train_set, by = "userId")
        # if nothing left, can already return NA
        if(length(validation_set) == 0) {
            return(NA)
        } else {
            # otherwise estimate ratings as \mu + b_i + b_u
            ratings_hat <- 
                validation_set %>% 
                left_join(b_i_tbl, by = "movieId") %>%
                left_join(b_u_tbl, by = "userId") %>%
                mutate(pred = mu + b_i + b_u) %>%
                .$pred
            #  and finally, calculate RMSE
            return(RMSE(ratings_hat, validation_set$rating))
            }
        })
    return(rmse_v)
    }
```

Then the RMSE estimation is repeated for several values of 
$\lambda = 0, 0.25, \ldots, 10.0$:

```{r reg movie user tuning, eval = FALSE}
# define sequence of $\lambda = 0, 0.25, \ldots, 10.0$
lambdas <- seq(0, 10, 0.25)
# calculate RMSEs for all these values of $\lambda$
RMSE_data_mur <- map_df(lambdas, function(lambda) {
    # calculate vector of RMSEs
    rmse_vec <- RMSE_movieuser_kfold(edx, index_list, lambda)
    # strip out the NA values
    rmse_vec <- na.omit(rmse_vec)
    # calculate mean and standard deviation of RMSEs
    list(RMSE_avg = mean(rmse_vec),
         RMSE_sd = sd(rmse_vec))
    })

# add lambda as first columns
RMSE_data_mur <- cbind(data.frame(lambda = lambdas), 
                       RMSE_data_mur)

# look up lambda for which the RMSE is minimal
lambda_mur_opt <- lambdas[which.min(RMSE_data_mur$RMSE_avg)]
RMSE_lambda_opt <- RMSE_data_mur$RMSE_avg[which.min(RMSE_data_mur$RMSE_avg)]
```

Note that regularisation RMSE data are loaded from the file
`RMSE_data_mur.Rdata` generated by the R code that accompanies this report, as
re-running the tuning takes several hours.

```{r load reg movie user RMSE, include = FALSE}
load(file = "RMSE_data_mur.Rdata")
```

Plotting the RMSE for these values of $\lambda$ shows that the optimal
values is $\lambda_{mur} =$  `r lambda_mur_opt`. 

```{r reg movie user plot}
RMSE_data_mur %>%
    ggplot(aes(x = lambda, y = RMSE_avg)) +
    geom_point() +
    geom_vline(xintercept = lambda_mur_opt, linetype = "dashed") +
    labs(y = "RMSE (with bootstrapped error)", 
         title = "Tuning of movie/user lambda for regularisation")
```

Finally, since the optimal values of $\lambda$ has been determined, the
prediction function is implemented:

```{r estimator reg movie user}
#' Prediction function for regularised movie/user effect
#' 
#' @param newdata Data for which to predict the ratings.
#' @param lambda Regularisation parameter $\lambda$.
#' @return Predicted ratings
predict_regmovieuser <- function(newdata, lambda) {
    # overall mean on training data
    mu <- mean(edx$rating)
    # tabulate movie effects "b_i" with regularisation
    b_i_tbl <- edx %>%
        group_by(movieId) %>%
        summarise(b_i = sum(rating - mu)/(n()+lambda))
    # tabulate user effects "b_u" with regularisation
    b_u_tbl <- edx %>% 
        left_join(b_i_tbl, by="movieId") %>%
        group_by(userId) %>%
        summarise(b_u = sum(rating - b_i - mu)/(n()+lambda))
    # calculate rating prediction by looking up "b_i" and "b_u"
    # from the tables just created and compute on the NEW data
    # \mu + b_i + b_u
    pred_ratings <- 
        newdata %>% 
        left_join(b_i_tbl, by = "movieId") %>%
        left_join(b_u_tbl, by = "userId") %>%
        mutate(pred = mu + b_i + b_u) %>%
        .$pred
    return(pred_ratings)
    }
```


### Additional Time Effect
After movie and user effect had been stripped out, an additional time effect
was observed in [Section 3.4.4](data_time_effect). The model then becomes
\[
Y_{u,i} = \mu + b_i + b_u + f(d_{u,i}) + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
where $d_{u,i}$ is the date rounded to weeks of the rating and $f$ is a 
smooth function. Since the plotting already used LOESS, the same method will
be used here, see also [Section 3.1.2](#method_loess) 
The model information for the `gamLoess` method 

```{r}
modelLookup("gamLoess")
```

says that `span` and `degree` can be tuned. The degree of the local regression
is fine to leave at 1 (linear), and the span will be varied. The tuning of the
`span` parameters is achieved with the `train` function of the `caret` 
package, using the values for $d_{u,i}$ calculated in 
[Section 3.4.4.2](#data_time_effect_res).

```{r train loess, eval = FALSE}
train_loess <- train(d_ui ~ rating_wk, data = d_ui_tbl, 
                     method = "gamLoess",
                     tuneGrid = data.frame(degree = 1,
                                           span = seq(0.10, 0.60, 0.05)))
```

Note that since this tuning takes some time, the results are stored for this
report and loaded from the file

```{r load time effect model, include = FALSE}
load(file = "train_loess.Rdata")
```

The optimal span parameters is $span_opt =$  `r train_loess$bestTune$span`
as can be seen from the plot of the RMSE against this parameter:

```{r time effect plot}
# plot and look up best span parameter
ggplot(train_loess, highlight = TRUE)
span_opt <- train_loess$bestTune$span

```

That the time effect is smoothed is illustrated by plotting the residual

```{r }
# plot fit of best model to time effect
d_ui_tbl %>%
    mutate(d_ui_hat = predict(train_loess, newdata = .)) %>%
    ggplot(aes(x = as.numeric(rating_wk))) +
    geom_point(aes(y = d_ui), alpha = 0.5) +
    geom_line(aes(y = d_ui_hat), col = "blue", size = 2) +
    ylim(c(-0.1, 0.1)) +
    labs(title = "Additional time effect fitted with LOESS",
         x = "Weeks since 1 Sep 1995",
         y = "Rating Effect")

# fit model for prediction of RESIDUAL time effect
fit_loess <- train_loess$finalModel

```




### Additional Genre Effect


# Results {#results}
Report on RMSE here


# Conclusion {#conclusion}
[TBD: write conclusion]

\newpage
\fontsize{14}{16}\selectfont{\textbf{Appendix A - Code of Figures}}

\hypertarget{appendix_figures} To enhance readibility of the report, the code
for most figures in [Section 3.4](methods_data_exp_vis) is shown in this
Appendix.

\fontsize{12}{14}\selectfont{}
\textbf{Figures in Section 3.4.1 - General Data Distribution}

\hypertarget{code_sparsity_figure}
Code for *data sparsity* figure shown [here](#data_sparsity_figure):

```{r code data matrix of subset, eval = FALSE}
edx_1000 %>%
    mutate(rating = 1) %>%
    select(movieId, userId, rating) %>%
    spread(movieId, rating) %>% 
    column_to_rownames(var = "userId") %>% 
    as.matrix() %>% t() %>%
    image(x = 1:n_movies_sub, y = 1:n_users_sub, z = ., 
          xlab = "Movies", ylab = "Users",
          col = grey.colors(n = 1, start = 0, end = 1))
```

\hypertarget{code_ratings_by_movie_figure}
Code for *ratings counts by movie* figure shown 
[here](#data_ratings_by_movie_figure):
```{r code rating by movie, eval = FALSE}
edx %>%
    group_by(movieId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black") +
    labs(title = "Rating counts by movie", 
         x = "Number of Ratings (log scale)")
```

\hypertarget{code_ratings_by_user_figure}
Code for *rating counts by user* figure shown 
[here](#data_ratings_by_user_figure):
```{r code rating by user, eval = FALSE}
edx %>%
    group_by(userId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 40, col = "black") +
    labs(title = "Rating counts by user", 
         x = "Number of Ratings (log scale)")
```

\hypertarget{code_ratings_distribution_figure}
Code for *distribution of ratings - total* histogram shown [here](#data_ratings_distribution_figure):
```{r code rating histogram, eval = FALSE}
edx %>%
    count(rating) %>%
    ggplot(aes(x = factor(rating), y = n)) +
    geom_bar(stat = "identity", width = 1, col = "black")+
    labs(title = "Distribution of ratings - total",
         x = "Rating", y = "Count") 
```

\textbf{Figure in Section 3.4.2 - Movie Effect}

\hypertarget{code_dist_ratings_by_movie}
Code for *movie effect* chart shown [here](#data_movie_effect):
```{r code dist rating by movie, eval = FALSE}
# distribution of ratings by movie
movie_avgs <- edx %>%
    group_by(movieId) %>%
    summarise(movie_avg = mean(rating),
              movie_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_bymovie = n()) %>%
    arrange(movie_avg) %>%
    mutate(row = row_number(movie_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_avg)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by movie")
p2 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_sd)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```

\textbf{Figure in Section 3.4.3 - User Effect}

\hypertarget{code_dist_ratings_by_user}
Code for *user effect* chart shown [here](#data_user_effect):
```{r code dist rating by user, eval = FALSE}
# distribution of ratings by user - in appendix
user_avgs <- edx %>%
    group_by(userId) %>%
    summarise(user_avg = mean(rating),
              user_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_byuser = n()) %>%
    arrange(user_avg) %>%
    mutate(row = row_number(user_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- user_avgs %>%
    ggplot(aes(x = row, y = user_avg)) +
    geom_point() +
    labs(x = "User (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by user")
p2 <- user_avgs %>%
    ggplot(aes(x = row, y = user_sd)) +
    geom_point() +
    labs(x = "User (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```

\textbf{Figures in Section 3.4.4 - Time of Rating}

\hypertarget{code_ratings_by_time}
Code for *time effect* chart shown [here](#data_time_effect_total):
```{r code rating by time, eval = FALSE}
# tabulate rating by time
 edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    group_by(ratingdate_wk) %>%
    summarize(rating_wk = mean(rating))
    ggplot(aes(x = ratingdate_wk, y = rating_wk)) +
    geom_point() +
    geom_smooth()
```

\hypertarget{code_ratings_by_time_res}
Code for *residual time effect* once movie and user effect are stripped out,
in chart shown [here](#data_time_effect_res):
```{r code residual time effect, eval = FALSE}
min_date = min(edx$ratingdate)
lambda_mur <- 5.0
# Calculate regularised movie user and time effect, see 
mu <- mean(edx$rating)
# Movie effect
b_i_tbl <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
# User effect
b_u_tbl <- edx %>% 
    left_join(b_i_tbl, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
# Residual time effect
d_ui_tbl <- edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
    left_join(b_i_tbl, by = "movieId") %>%
    left_join(b_u_tbl, by = "userId") %>%
    group_by(rating_wk) %>%
    summarise(d_ui = mean(rating - mu - b_i - b_u))

# plot the overall time effect once movie and user effect are accounted for
d_ui_tbl %>%
    ggplot(aes(x = as.numeric(rating_wk), y = d_ui)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Residual time effect of ratings",
         x = paste0("Weeks since ",format(min_date, "%d %b %Y")),
         y = "Residual rating effect (stars)")
```


\textbf{Figures in Section 3.4.5 - Genre Effect}

\hypertarget{code_nratings_by_genre}
The code for the first chart in [Section 3.4.5](data_genre_effect) is:
```{r code genre number of ratings, eval = FALSE}
edx %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              nratings = n()) %>%
    arrange(nratings) %>%
    ggplot(aes(x = nratings)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black")
```

\hypertarget{code_ratings_by_genre}
The code for the second chart is:
```{r code ratings by genre, eval = FALSE}
# First chart - average ratings by genres, in ascending order
p1 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_avg)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "Ratings by genres (with at least 1000 ratings)",
         x = "Genre (sorted by average rating)",
         y = "Average Rating")

# Second chart - standard dev of ratings by genre, genres need to
# be in same order as in first chart
p2 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_sd)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "",
         x = "Genre (sorted by average rating)",
         y = "Rating Standard Deviation")

grid.arrange(p1, p2, nrow = 2)
```
