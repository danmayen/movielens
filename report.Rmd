---
title: "MovieLens Project"
author: "Dr Daniel Mayenberger"
date: "May 2020"
header-includes:
   - \usepackage{hyperref}
   - \usepackage[usenames,dvipsnames]{xcolor}
output: 
    pdf_document:
        number_sections: true
---

```{r setup, include=FALSE, echo=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r libraries and load data, include = FALSE}
# libraries
library(tidyverse)
library(lubridate)
library(caret)
library(ggplot2)
library(gridExtra)
# Set to TRUE for final version, is kept at FALSE for quicker compiling
# of drafts
FINAL_VERSION = FALSE

# load data from local repository
if(FINAL_VERSION) {
    source("loadLocalData.R")
} else {
    source("loadLocalSampleData.R")
    edx <- edx_1000
    validation <- validation_1000
    }
```

\hypersetup {
    linkcolor=green
    urlcolor=cyan
    }

# Executive Summary
Given over 10,000 movies and about 70,000 users that have rated some of these
movies on a rating scale from zero to five stars, the goal is to predict future
ratings based on past ratings. The challenge is to do this for the over 700
millions different movie/user combinations  based on only 1.2% (9 millions) of
such combinations used for training the algorithm.

The available data of 10 million ratings are split into 9 million ratings to
train different algorithms and 1 million ratings to evaluate the performance
of these methods. Performance is the root-mean squared error (RMSE) of ratings.

Based on the RMSE, the model that performs best with an RMSE of [TBD] is
\[
Y_{u,i} = \mu + b_i + b_u + \sum_{k=1}^K \delta_{i,k} \beta_k + 
\varepsilon_{u,i},
\]
where $Y_{u,i}$ is the rating by user $u$ of movie $i$, and $\delta_{i,k}$ is
an indicator which is $1$ if the movie $i$ is of genre $k$ and zero otherwise.

# Introduction
The movielens project makes available millions of a movie ratings provided by
anonymised users.

From the movielens website information about the data [here](http://files.grouplens.org/datasets/movielens/ml-10m-README.html)
the following further information is provided: 

* The data set contains just over 10 million ratings by users of the online
movie recommender service MovieLens.
* Users were selected at random for inclusion. All users selected had rated at
least 20 movies. Each user is represented by an id, and no other information is
provided.

Denoting the rating by any of these users $u$ of a certain movie $i$, the task
is to predict ratings $Y_{u,i}$ of any such user/movie combination $(u,i)$. The
challenge is that there are overall 71,567 users and 10,681 movies, yielding
$71,567 \times 10,681 = 764,407,127$ while only 10 million data points, so only
about 1.3% $(=10/764.4)$ such data are available in totality and 10% (1 million)
of the data must be held out as test set, leaving 1.2% $(=9/764.4)$ of data to
train a rating prediction algorithm.

To do so, the data are examined and modelled in [**Section 3**](#methods) which is
further broken down into

* [Section 3.1](#methods_techniques) to elaborate on the techniques used, in 
particular in the modelling [Section 3.5](#methods_modelling).
* [Section 3.2](#methods_data_structure) to provide an overview of the basic
structure of the rating data.
* [Section 3.3](#methods_data_cleaning) to perform data cleaning.
* [Section 3.4](#methods_data_exp_vis) to visualise the most important 
properties of the ratings with respect to individual movies, their genres
or individual users. These properties then inspire the modelling methods
in the subseqeuent section.
* [Section 3.5](#methods_modelling) presents the models based on the most
salient data properties and calibrates any free modelling parameters.

The results of all models are summarised in [Section 4](#results) and the
conclusions are drawn in [Section 5](#conclusion).

# Methods and Analysis {#methods}

## Process and Techniques Used {#methods_techniques}

Two modelling techniques will be described, first the regularisation in 
[Section 3.1.1](#method_regularisation) and then the local estimated scatterplot 
smoothing (LOESS) method in [Section 3.1.2](#method_loess).

### Regularisation {#method_regularisation}
The modelling techniques employed are regularised least squared estimates.
Specifically, a model of the form
\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\]
for user $u$ and movie $i$ assumes that there is an overall average rating of
$\mu$ across all movies. The $b_i$ is based on the observation that some movies
are better rated than others, as will be shown in [Section
3.4.2](#data_movie_effect). 
However, as there are some movies that are rated by hundreds or more users, the
ratings of these movies are more reliable than the ratings of movies that are
only rated by a few users. To calculate such an effect, denote the number of
ratings awarded to movie $i$ by $n_i$, the overall number of ratings as $N$ and
the overall number of movies by $J$. Further, denote the overall average rating
with $\mu$.

Without any weighing of ratings for a movie by the number of ratings awarded to
it, the goal is to minimise the MSE given by
\[
MSE = \frac{1}{N} \sum_{i=1}^J \sum_{u=1}^{n_i} (y_{u,i} - \hat{y}_{u,i})^2.
\]
As for any user $u$ and movie $i$ the modelled rating is given by 
$\hat{y}_{u,i} = \mu + \hat{b}_i$, the MSE to minimise becomes
\[
MSE = \frac{1}{N} \sum_{i=1}^J \sum_{u=1}^{n_i} 
\left( y_{u,i} - (\mu + \hat{b}_i) \right)^2.
\]
The necessary condition for a minimum is $\frac{\partial MSE}{\partial b_j}=0$
for all $j=1,2, \ldots, J$ which leads to
\[
0 = -\frac{2}{N} \sum_{u=1}^{n_j} \left( y_{u,j} - (\mu + \hat{b}_j) \right)
\quad (j=1,\ldots,J).
\]
This is equivalent to
\[
\hat{b}_j = \frac{1}{n_j} \sum_{u=1}^{n_j}(y_{u,j} - \mu) \quad (j=1,\ldots,J).
\]
To penalise estimates for movies that are only rated by a few users, the
MSE to minimise then becomes
\[
MSE(\lambda) = \frac{1}{N} \left[
\sum_{i=1}^J \sum_{u=1}^{n_i} (y_{u,i} - \hat{y}_{u,i})^2 +
\lambda \sum_{i=1}^J b_i^2
\right]
\]
with a regularisation parameter $\lambda \geq 0$. Again, the necessary condition
for a minimum of MSE is $\frac{\partial MSE(\lambda)}{\partial b_j}=0$
for all $j=1,2, \ldots, J$ which implies
\[
0 = -\sum_{u=1}^{n_j} \left( y_{u,j} - (\mu + \hat{b}_j) \right) +
\lambda b_j
\quad (j=1,\ldots,J).
\]
Solving for $\hat{b}_j$ yields
\[
\hat{b}_j = \frac{1}{n_j+\lambda} 
\sum_{u=1}^{n_j}(y_{u,j} - \mu) \quad (j=1,\ldots,J).
\]
This solution is a minimum, since the second derivative (Hessian matrix) of the
MSE function is a diagonal matrix, as 
$\frac{\partial^2MSE}{\partial b_i \partial b_j} = 0$ for $i \neq j$ and
$\frac{\partial^2MSE}{\partial b_i^2} = 2\frac{n_i+\lambda}{N} > 0$. There are
only positive entries on the diagonal of this matrix, and with that it is
positively definitive. So the solution for $\hat{b}_j$ provided above is
a local minimum and as there are no other local optima (there is only one
point at which the first partial derivative vanishes), it is also a global
minimum.

### Local Estimated Scatterplot Smoothing (LOESS) {#method_loess}
Unlike standard local regression that fits a line to the whole data set,
the locally estimated scatterplot smoothing (LOESS) estimates a regression
line in a local window that is progressively moved through the data. The
parameter of LOESS is the width of this window.

## Data Structure and Loading {#methods_data_structure}
The raw data is provided in two sets, called `edx` and `validation`.

### Sample Data {#data_sample_data}
To facilitate code verification, 1,000 samples of data have been extracted from
both datasets `edx` and `validation`, called `edx_1000` and `validation_1000`
respectively. This is achieved with the following code:

```{r data sample data, eval = FALSE}
set.seed(123, sample.kind = "Rounding")
edx_1000 <- sample_n(edx, size = 1000)
validation_1000 <- sample_n(validation, size = 1000)
```

### Basic Data Structure
Both `edx` and `validation` have the same structure that can be displayed with 
the `str` function and is listed below for `edx`:

|Column name   |Type      |First values                                                                                     |
|:---------|:---------|:-----------------------------------------------------------------------------------------------|
|userId    |integer   |1 1 1                                                                                           |
|movieId   |numeric   |122 185 292                                                                                     |
|rating    |numeric   |5 5 5                                                                                           |
|timestamp |integer   |838985046 838983525 838983421                                                                   |
|title     |character |Boomerang (1992) Net, The (1995) Outbreak (1995)                                                |
|genres    |character |Comedy&#124;Romance Action&#124;Crime&#124;Thriller Action&#124;Drama&#124;Sci-Fi&#124;Thriller |


The data sets are both tidy as well - the count of `NA` values is zero for both 
sets:
```{r data tidy}
edx %>% summarise_all(~sum(is.na(.)))
validation %>% summarise_all(~sum(is.na(.)))
```

The `userId`, `movieId` , `rating` and `title` data can be used directly in
the format provided. These two columns need further processing:

1. The `timestamp` column is in raw data format of seconds since 1 January 1970 
and will be converted into a date & time format.
2. The `genres` column contains one ore more genres for each movie, separated
by a pipe (`|`) symbol, so they will be separated.

## Data Cleaning {#methods_data_cleaning}
### Conversion of Timestamp to Date
The `timestamp` column is converted to a date and time using the `as_datetime`
function for both the `edx` and the `validation` dataset and stored in 
a new column `ratingdate`. After the conversion the `timestamp` columns
are no longer required and are discarded.
```{r data conversion of date}
edx <- edx %>%
    mutate(ratingdate = as_datetime(timestamp)) %>%
    select(-timestamp)

validation <- validation %>%
    mutate(ratingdate = as_datetime(timestamp)) %>%
    select(-timestamp)
```

### Separation of Genres
[TBD once needed]

## Data Exploration and Visualisation {#methods_data_exp_vis}

### General Data Distribution Properties
The `edx` data set contains 9,000,055 data point, consisting of 10,677
distinct movies and 69,878 different users:
```{r data count}
edx %>%
    summarise(n_movies = n_distinct(movieId),
              n_users = n_distinct(userId))
```
With that, the total number of diferent movie/user combinations is
$10,677 \times 69,878 = 746,087,406$, much more than the overall 10 million
data points from the combined `edx` and `validation` sets. While it is not
feasible with the memory capacity of a personal computer to visualise all of
the over 700 million combinations, it is possible to illustrate the sparsity
of the data set on a representative sample.

```{r, include = FALSE}
n_movies_sub <- length(unique(edx_1000$movieId))
n_users_sub <- length(unique(edx_1000$userId))
```

For better readability, the code for the subsequent graphs in this Section
are displayed in \hyperlink{appendix_figures}{Appendix A}.

#### Data Sparsity {#data_sparsity_figure}
Within the sample of 1,000 ratings drawn as described in 
[Section 3.2.1](#data_sample_data) there are already `r n_movies_sub`
different movies and `r n_users_sub` different users. Plotting the
data points of this matrix shows that the data given are very sparse 
but at least evenly distributed among movies and users.

```{r data matrix of subset, echo = FALSE}
edx_1000 %>%
    mutate(rating = 1) %>%
    select(movieId, userId, rating) %>%
    spread(movieId, rating) %>% 
    column_to_rownames(var = "userId") %>% 
    as.matrix() %>% t() %>%
    image(x = 1:n_movies_sub, y = 1:n_users_sub, z = ., 
          xlab = "Movies", ylab = "Users",
          col = grey.colors(n = 1, start = 0, end = 1))
```

The code is shown \hyperlink{code_sparsity_figure}{here}.

#### Ratings by Movie {#data_ratings_by_movie_figure}

There is a wide variety of the number of ratings by movie. Some movies are
rated only up to 10 times, while others are rated thousands of times:

```{r rating by movie, echo = FALSE}
edx %>%
    group_by(movieId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black") +
    labs(title = "Rating counts by movie", 
         x = "Number of Ratings (log scale)")
```

The code is shown \hyperlink{code_ratings_by_movie_figure}{here}.

#### Ratings by User {#data_ratings_by_user_figure}

Similarly, there are users that only rate a few movies while other rate
hundreds or even thousands of movies:

```{r rating by user, echo = FALSE}
edx %>%
    group_by(userId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 40, col = "black") +
    labs(title = "Rating counts by user", 
         x = "Number of Ratings (log scale)")
```

The code is shown \hyperlink{code_ratings_by_user_figure}{here}.

#### Distribution of Ratings {#data_ratings_distribution_figure}

The ratings most awared are three and four stars. In general, whole-star
ratings are more frequently given than half-star ratings:

```{r rating histogram, echo = FALSE}
edx %>%
    count(rating) %>%
    ggplot(aes(x = factor(rating), y = n)) +
    geom_bar(stat = "identity", width = 1, col = "black")+
    labs(title = "Distribution of ratings - total",
         x = "Rating", y = "Count") 
```

The code is displayed \hyperlink{code_ratings_distribution_figure}{here}.

### Movie Effect {#data_movie_effect}
From public movie ratings such as [Rotten
Tomatoes](https://www.rottentomatoes.com/) it is known that some movies are in
general better rated than others. We group the ratings by movie ID to 
evaluate the same effect. In addition to the average rating for each movie,
the standard deviation of ratings for the same movie is visualised in parallel.

```{r dist rating by movie, echo = FALSE}
# distribution of ratings by movie
movie_avgs <- edx %>%
    group_by(movieId) %>%
    summarise(movie_avg = mean(rating),
              movie_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_bymovie = n()) %>%
    arrange(movie_avg) %>%
    mutate(row = row_number(movie_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_avg)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by movie")
p2 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_sd)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is \hyperlink{code_dist_ratings_by_movie}{here}.

So there are indeed movies that, on average:

* are rated worse than other movies, on the left side of the chart.
* are rated better than other movies, towards the right of the chart.

At the same time, there is a high variability of this *movie effect*, 
illustrated by the broad range of the standard deviation in the 
lower half of the figure. Standard deviation of ratings by movie hovers around
the value of one (star).

### User Effect {#data_user_effect}
Similar to the movie effect, different users may have the tendency to award
higher or lower ratings, compared to other users.

```{r dist rating by user, echo = FALSE}
# distribution of ratings by user
user_avgs <- edx %>%
    group_by(userId) %>%
    summarise(user_avg = mean(rating),
              user_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_byuser = n()) %>%
    arrange(user_avg) %>%
    mutate(row = row_number(user_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- user_avgs %>%
    ggplot(aes(x = row, y = user_avg)) +
    geom_point() +
    labs(x = "User (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by user")
p2 <- user_avgs %>%
    ggplot(aes(x = row, y = user_sd)) +
    geom_point() +
    labs(x = "User (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is \hyperlink{code_dist_ratings_by_user}{here}.

So similarly to the movie effect, there is a *user effect* with users that 
tend to assign movies lower rating on the left hand side of the chart and
users that rate movies more highly towards the right. In constrast to the
*movie effect* there is much higher variability of rating, consistent with
users differentiating between good and bad movies.


### Time of Rating {#data_time_effect}
Another feature that may influence the rating is the date and time at which
the rating was awarded. When averaging all ratings within a week and following
this average with a LOESS function, the following pattern emerges:

#### Overall Time Effect {#data_time_effect_total}
```{r rating by time, echo = FALSE}
edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    group_by(ratingdate_wk) %>%
    summarise(rating_wk = mean(rating)) %>%
    ggplot(aes(x = ratingdate_wk, y = rating_wk)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Rating by date",
         x = "Rating date (granularity of weeks)",
         y = "Rating")
```
  
The code for this chart is \hyperlink{code_ratings_by_time}{here}.

So there is a small effect that is worth capturing, as a smooth function
of time. 

#### Residual Time Effect {#data_time_effect_res}

Since the effect will be modelled after movie effect and user
effect are accounted for, it is also informative to plot the residual
effect.

```{r residual time effect, echo = FALSE}
min_date = min(edx$ratingdate)
lambda_mur <- 5.0
# Calculate regularised movie user and time effect, see 
mu <- mean(edx$rating)
# Movie effect
b_i_tbl <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
# User effect
b_u_tbl <- edx %>% 
    left_join(b_i_tbl, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
# Residual time effect
d_ui_tbl <- edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
    left_join(b_i_tbl, by = "movieId") %>%
    left_join(b_u_tbl, by = "userId") %>%
    group_by(rating_wk) %>%
    summarise(d_ui = mean(rating - mu - b_i - b_u))

# plot the overall time effect once movie and user effect are accounted for
d_ui_tbl %>%
    ggplot(aes(x = as.numeric(rating_wk), y = d_ui)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Residual time effect of ratings",
         x = paste0("Weeks since ",format(min_date, "%d %b %Y")),
         y = "Residual rating effect (stars)")

```

The code for this figure can be found 
\hyperlink{code_ratings_by_time_res}{here}.

In comparison to the total effect, the residual time effect is:

* much stronger in the first 2 years, after `r format(min_date, "%d %b %Y")`.
* considerable flatter after the the first 2 years.

### Genre Effect {#data_genre_effect}

As final feature, we investigate whether certain genres (or their combinations)
are rated differently from others. First, we determine whether genres have
sufficiently many ratings by examining the distribution of the number of
ratings by genre:

```{r genre number of ratings, echo = FALSE}
edx %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              nratings = n()) %>%
    arrange(nratings) %>%
    ggplot(aes(x = nratings)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black")
```

The code for this figure is shown \hyperlink{code_nratings_by_genre}{here}.

So there are definitely sufficiently many genres with 1,000 or more ratings.
In the same way as for movies and users, we group the rating by genres for
all genres with over 1,000 rating and chart their average rating and standard 
deviation.

```{r ratings by genre, echo = FALSE}
p1 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_avg)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "Ratings by genres (with at least 1000 ratings)",
         x = "Genre (sorted by average rating)",
         y = "Average Rating")

p2 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_sd)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "",
         x = "Genre (sorted by average rating)",
         y = "Rating Standard Deviation")

grid.arrange(p1, p2, nrow = 2)
```
The code for this chart is shown \hyperlink{code_ratings_by_genre}{here}.

So the genre combination does have an effect on the rating, in a similar way
as movies and users do.

### Extrapolation Requirements Check
To predict movie ratings and calculate the final RMSE, it must be considered 
how the algorithm handles data in the `validation` data set to which it was
not trained in the `edx` set. It will be shown that there is no need for 
such extrapolation for the provided data sets.

By construction, the `validation` set only contains movies and users that 
are also in the `edx` set, with the `semi_join` statements provided in
the instructions. For verification, this code confirms that indeed no movies
or users from the `validation` set are missing from the `edx` set:

```{r}
validation %>%
    anti_join(edx, by = "movieId") %>%
    group_by(movieId, title) %>%
    summarise(n = n())

validation %>%
    anti_join(edx, by = "userId") %>%
    group_by(userId) %>%
    summarise(n = n())
```

In addition, all genres, and even more specifically, all their combinations
that are present in the `validation` set are also found in the `edx` set, 
as this code shows:
```{r}
validation %>%
    anti_join(edx, by = "genres") %>%
    group_by(genres) %>%
    summarise(n = n())
```
So with the given data set, no extrapolation to movies, users or genres needs
to be made in predicting ratings and determining model performance.

## Modelling Approach {#methods_modelling}
The training of model parameters is generally done using k-fold 
cross-validation. To do this, the `edx` data is split into a training and
a validation set $k$ times. 

Graphically, k-fold validation can be illustrated by $k$ subsequent splits of
the overall `edx` set into training sets (shown in blue) and test sets (shown in
purple).

```{r kfold crossvalidation, echo = FALSE}
n_data <- nrow(edx)
validation_split <- data.frame(
    k = seq(1, 25, 1),
    C = seq(0, 24, 1) / 25 * n_data,
    B = rep(1/25 * n_data, time = 25))
validation_split <- validation_split %>%
    mutate(A = n_data - B - C)
validation_split <- validation_split %>%
    gather(set, value, -k) 

validation_split %>%
    ggplot(aes(x = k, y = value, fill = set)) +
    scale_fill_manual(values = c("#ACE5EE", "#FAF0BE", "#ACE5EE")) +
    theme(plot.margin = margin(0,0,0,0, "cm")) +
    geom_bar(stat = "identity", position = "stack", show.legend = FALSE) +
    labs(title = "Illustration of k-fold cross validation (k = 25)",
         y = "Values")
```
\definecolor{blizzardblue}{rgb}{0.67, 0.9, 0.93}
\definecolor{blond}{rgb}{0.98, 0.94, 0.75}

For example, the parameter $\lambda$ is applied to the
\colorbox{blizzardblue}{training data} and the RMSE that results from that
particular values of $\lambda$ is evaluated in the 
\colorbox{blond}{test data}.

The **performance** of each algorithm is evaluated using the RMSE which is
implemented in the function `RMSE_rating`:

```{r RMSE function}
RMSE_rating <- function(pred_ratings, true_ratings) {
    ifelse(length(pred_ratings) > 0,
           sqrt(mean((pred_ratings - true_ratings)^2)),
           NA)
    }
```

### Constant Value

This is the simplest baseline model that assumes that the rating is the
same every time. The model is
\[
Y_{u,i} = \mu + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
The parameter $\mu$ is estimated as average across all ratings:
\[
\hat{\mu} = \frac{1}{N} \sum_{u,i} y_{u,i}.
\]

The R implementation of this model is given by a function that returns the
same value for each record of the requested data set:
```{r estimator const}
predict_const <- function(newdata) {
    mu <- mean(edx$rating)
    return(rep(mu, nrow(newdata)))
    }
```


### Movie Effect Only
As seen in [Section 3.4.2](#data_movie_effect), there is an effect of 
individual movies on the rating. In terms of the model,
\[
Y_{u,i} = \mu + b_i + \varepsilon_{u,i}
\qquad (i=1,\ldots,J; u=1,\ldots,n_i),
\]
where $b_i$ is the effect (or bias) of movie $i$ across all users,
and the residual values is $\varepsilon_{u,i}$.

### User Effect Only
We observed in [Section 3.4.3](#data_user_effect) an effect of users on the 
rating.

### Combined Movie and User Effect


### Regularised Movie and User Effect


### Additional Time Effect

### Additional Genre Effect


# Results {#results}
Report on RMSE here


# Conclusion {#conclusion}
[TBD: write conclusion]

\newpage
\fontsize{14}{16}\selectfont{\textbf{Appendix A - Code of Figures}}

\hypertarget{appendix_figures} To enhance readibility of the report, the code
for most figures in [Section 3.4](methods_data_exp_vis) is shown in this
Appendix.

\fontsize{12}{14}\selectfont{}
\textbf{Figures in Section 3.4.1 - General Data Distribution}

\hypertarget{code_sparsity_figure}
Code for *data sparsity* figure shown [here](#data_sparsity_figure):

```{r code data matrix of subset, eval = FALSE}
edx_1000 %>%
    mutate(rating = 1) %>%
    select(movieId, userId, rating) %>%
    spread(movieId, rating) %>% 
    column_to_rownames(var = "userId") %>% 
    as.matrix() %>% t() %>%
    image(x = 1:n_movies_sub, y = 1:n_users_sub, z = ., 
          xlab = "Movies", ylab = "Users",
          col = grey.colors(n = 1, start = 0, end = 1))
```

\hypertarget{code_ratings_by_movie_figure}
Code for *ratings counts by movie* figure shown 
[here](#data_ratings_by_movie_figure):
```{r code rating by movie, eval = FALSE}
edx %>%
    group_by(movieId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black") +
    labs(title = "Rating counts by movie", 
         x = "Number of Ratings (log scale)")
```

\hypertarget{code_ratings_by_user_figure}
Code for *rating counts by user* figure shown 
[here](#data_ratings_by_user_figure):
```{r code rating by user, eval = FALSE}
edx %>%
    group_by(userId) %>%
    summarise(n = n()) %>%
    ggplot(aes(x = n)) +
    scale_x_log10() +
    geom_histogram(bins = 40, col = "black") +
    labs(title = "Rating counts by user", 
         x = "Number of Ratings (log scale)")
```

\hypertarget{code_ratings_distribution_figure}
Code for *distribution of ratings - total* histogram shown [here](#data_ratings_distribution_figure):
```{r code rating histogram, eval = FALSE}
edx %>%
    count(rating) %>%
    ggplot(aes(x = factor(rating), y = n)) +
    geom_bar(stat = "identity", width = 1, col = "black")+
    labs(title = "Distribution of ratings - total",
         x = "Rating", y = "Count") 
```

\textbf{Figure in Section 3.4.2 - Movie Effect}

\hypertarget{code_dist_ratings_by_movie}
Code for *movie effect* chart shown [here](#data_movie_effect):
```{r code dist rating by movie, eval = FALSE}
# distribution of ratings by movie
movie_avgs <- edx %>%
    group_by(movieId) %>%
    summarise(movie_avg = mean(rating),
              movie_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_bymovie = n()) %>%
    arrange(movie_avg) %>%
    mutate(row = row_number(movie_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_avg)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by movie")
p2 <- movie_avgs %>%
    ggplot(aes(x = row, y = movie_sd)) +
    geom_point() +
    labs(x = "Movie (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```

\textbf{Figure in Section 3.4.3 - User Effect}

\hypertarget{code_dist_ratings_by_user}
Code for *user effect* chart shown [here](#data_user_effect):
```{r code dist rating by user, eval = FALSE}
# distribution of ratings by user - in appendix
user_avgs <- edx %>%
    group_by(userId) %>%
    summarise(user_avg = mean(rating),
              user_sd = ifelse(n()>1,sd(rating),0),
              n_ratings_byuser = n()) %>%
    arrange(user_avg) %>%
    mutate(row = row_number(user_avg))

# plot average of ratings and their
# standard deviation, horizontally aligned
p1 <- user_avgs %>%
    ggplot(aes(x = row, y = user_avg)) +
    geom_point() +
    labs(x = "User (sorted by average rating)", 
         y = "Average Rating", title = "Ratings by user")
p2 <- user_avgs %>%
    ggplot(aes(x = row, y = user_sd)) +
    geom_point() +
    labs(x = "User (sorted by average rating)",
         y = "Rating Standard Deviation")
grid.arrange(p1, p2, nrow = 2)
```

\textbf{Figures in Section 3.4.4 - Time of Rating}

\hypertarget{code_ratings_by_time}
Code for *time effect* chart shown [here](#data_time_effect_total):
```{r code rating by time, eval = FALSE}
# tabulate rating by time
 edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    group_by(ratingdate_wk) %>%
    summarize(rating_wk = mean(rating))
    ggplot(aes(x = ratingdate_wk, y = rating_wk)) +
    geom_point() +
    geom_smooth()
```

\hypertarget{code_ratings_by_time_res}
Code for *residual time effect* once movie and user effect are stripped out,
in chart shown [here](#data_time_effect_res):
```{r code residual time effect, eval = FALSE}
min_date = min(edx$ratingdate)
lambda_mur <- 5.0
# Calculate regularised movie user and time effect, see 
mu <- mean(edx$rating)
# Movie effect
b_i_tbl <- edx %>%
    group_by(movieId) %>%
    summarise(b_i = sum(rating - mu)/(n()+lambda_mur))
# User effect
b_u_tbl <- edx %>% 
    left_join(b_i_tbl, by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u = sum(rating - b_i - mu)/(n()+lambda_mur))
# Residual time effect
d_ui_tbl <- edx %>%
    mutate(ratingdate_wk = round_date(ratingdate, unit = "week")) %>%
    mutate(rating_wk = (ratingdate_wk - min_date)/7) %>%
    left_join(b_i_tbl, by = "movieId") %>%
    left_join(b_u_tbl, by = "userId") %>%
    group_by(rating_wk) %>%
    summarise(d_ui = mean(rating - mu - b_i - b_u))

# plot the overall time effect once movie and user effect are accounted for
d_ui_tbl %>%
    ggplot(aes(x = as.numeric(rating_wk), y = d_ui)) +
    geom_point() +
    geom_smooth(formula = y ~ x, method = "loess") +
    labs(title = "Residual time effect of ratings",
         x = paste0("Weeks since ",format(min_date, "%d %b %Y")),
         y = "Residual rating effect (stars)")
```


\textbf{Figures in Section 3.4.5 - Genre Effect}

\hypertarget{code_nratings_by_genre}
The code for the first chart in [Section 3.4.5](data_genre_effect) is:
```{r code genre number of ratings, eval = FALSE}
edx %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              nratings = n()) %>%
    arrange(nratings) %>%
    ggplot(aes(x = nratings)) +
    scale_x_log10() +
    geom_histogram(bins = 30, col = "black")
```

\hypertarget{code_ratings_by_genre}
The code for the second chart is:
```{r code ratings by genre, eval = FALSE}
# First chart - average ratings by genres, in ascending order
p1 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_avg)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "Ratings by genres (with at least 1000 ratings)",
         x = "Genre (sorted by average rating)",
         y = "Average Rating")

# Second chart - standard dev of ratings by genre, genres need to
# be in same order as in first chart
p2 <- edx %>%
    select(genres, rating) %>%
    mutate(genres = reorder(genres, rating, mean)) %>%
    group_by(genres) %>%
    summarise(rating_avg = mean(rating),
              rating_sd = sd(rating),
              nratings = n()) %>%
    filter(nratings >= 1000) %>%
    ggplot(aes(x = genres, y = rating_sd)) +
    scale_y_log10() +
    theme(axis.text.x = element_text("")) +
    geom_point() +
    labs(title = "",
         x = "Genre (sorted by average rating)",
         y = "Rating Standard Deviation")

grid.arrange(p1, p2, nrow = 2)
```
